{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch\n",
    "\n",
    "Sciki-learn is a popular machine learning library, however, other popular libraries in this domain should not be ignored. While scikit-learn has its focus on traditional machine learning algorithms and is characterized by its uniform sytnax and purposes w.r.t. to preprocessing, model selection, model training and model evaluation, *pytorch* plays a more important role in the field of deep learning, especially for models which are build as neural network architectures. \n",
    "\n",
    "Pytorch provides tools for building and training complex neural network models, particularly those that benefit from GPU acceleration. Thus, it is suitable for applications in, e.g., computer vision, natural language processing, and reinforcement learning. In comparison to scikit-learn, it can be considered as a low-level library that provides more control and flexibility over model building and training. At the same time, it requires more code to set up and train models compared to scikit-learn, but this allows for greater customization.\n",
    "\n",
    "Its core blocks are:\n",
    "\n",
    "* Tensors: Fundamental data structure, similar to NumPy arrays but with GPU support.\n",
    "* Autograd: Automatic differentiation for building and training neural networks.\n",
    "* NN Module: High-level neural network API for constructing deep learning models.\n",
    "* Optim: Optimization algorithms (e.g., SGD, Adam) for training models.\n",
    "* Dynamic Computational Graphs: Graphs are built on-the-fly, allowing for flexible model design.\n",
    "\n",
    "By the sub-modules of the NN module, neural networks can be manually defined in very custom ways. Such complex models are usually trained by numerical optimization which is based on gradient information that is in need of differentiation. The functionality of automatic differentiation in combination with the availability of different optimization algorithms is one of the main reasons for the popularity of pytorch. Furthermore, pytorch is open-source and has a large and usually helpful community. \n",
    "\n",
    "Note that *tensorflow* is an equivalent option to pytorch. It offers more or less the same functionalities. Due to the more pythonic way how models are defined with pytorch and its wide spread usage in the research area, we are going to stick to pytorch.  So let us learn about the key concepts of pytorch.\n",
    "\n",
    "## Tensors\n",
    "\n",
    "Tensors are very similar to numpy arrays, however, they can run on the GPU and are optimized for automatic differentiation. Tensors can be created from data or from a numpy array (many other ways to create tensors do exist)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True],\n",
       "        [True, True]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "x = [[1, 2], [3, 4]]\n",
    "x_tensor = torch.tensor(x)\n",
    "x_array = np.array(x)\n",
    "x_tensor_from_np = torch.from_numpy(x_array)\n",
    "\n",
    "x_tensor == x_tensor_from_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attributes of tensors can be inferred similar to numpy arrays, e.g., the *shape* or *dtype* attribute. However, one difference is given by the *device* attribute which tells us if the tensor runs on the GPU or CPU. By default, it is the CPU. Given a GPU is available, the tensor can be transferred to the GPU with the *to* method. Depending on the architecture, the device name usually is \"cuda\" or \"mps\" for macbooks with M chips. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device at start: cpu\n",
      "Device after shift to GPU: mps:0\n",
      "The shape of the tensor is: torch.Size([2, 2])\n",
      "The data type of the tensor is: torch.int64\n"
     ]
    }
   ],
   "source": [
    "print(f\"Device at start: {x_tensor.device}\")\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "x_tensor = x_tensor.to(device)\n",
    "\n",
    "print(f\"Device after shift to GPU: {x_tensor.device}\")\n",
    "print(f\"The shape of the tensor is: {x_tensor.shape}\")\n",
    "print(f\"The data type of the tensor is: {x_tensor.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors come a long with a large number of operations which are mostly similar to operations which we know from numpy array. An overview can be found [here](https://pytorch.org/docs/stable/torch.html).\n",
    "\n",
    "## The neural network module\n",
    "\n",
    "Pytorch includes a neural network submodule which comes with pre-defined building blocks for neural networks which can be found [here](https://pytorch.org/docs/stable/nn.html#linear-layers). For instance, the *Linear* class defines an affine transformation of the form:\n",
    "\n",
    "$$\n",
    "f \\left( \\mathbf{x} \\right): \\mathbf{x}^T W + b\n",
    "$$\n",
    "\n",
    "where $\\mathbf{x}$ is the input which either is given by observable feature variables or by hidden neurons from a previous layer, $W, b$ are parameters. An instance of the *Linear* class initializes parameters randomly. See the equivalence of the class forward method and the manual processing below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.4774, 1.3887, 0.5240],\n",
       "        [2.7257, 3.2935, 1.6014]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "x_tensor = torch.tensor(x)\n",
    "x_tensor = x_tensor.to(torch.float32)\n",
    "linear = nn.Linear(in_features=2, out_features=3)\n",
    "linear(x_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.4774, 1.3887, 0.5240],\n",
       "        [2.7257, 3.2935, 1.6014]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tensor @ linear.weight.transpose(0, 1) + linear.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.0110, 0.6131],\n",
       "        [0.3060, 0.6464],\n",
       "        [0.4842, 0.0545]], requires_grad=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 0.2402, -0.2102, -0.0691], requires_grad=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks consist of layers which are themselves compositions of different functions. For instance, a fully connected layer composes the affine transformation from above with an activation function $g$. Activation functions can be chosen as desired and an overview of pytorch implementations can be found [here](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity). Using $l$ to denote the layer $l$ of a neural network, it can be seen as a large number of different functional compositions:\n",
    "\n",
    "$$\n",
    "g^{(L)} \\left( f^{(L)} \\left( g^{(L-1)} \\left( f^{(L-1)} \\left( ... g^{(2)} \\left( f^{(2)} \\left( g^{(1)} \\left( f^{(1)} \\left( \\mathbf{x} \\right) \\right) \\right) \\right)  \\right) \\right) \\right) \\right) \n",
    "$$\n",
    "\n",
    "This structure can easily be defined using pytorch's nn.Module class. The layers are defined within the *\\_\\_init\\_\\_* method of the class and a *forward* method defines how input is processed through the network. The *nn.Sequential* is a useful class which creates a container of all operations which are defined by the network. First, let us take a look at a simple example which defines a forward neural network for a regression task which includes a hidden layer with a ReLu activation function. The number of input dimensionality (number of input features) and the number of hidden neurons must be set at initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class RegressionNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dimension, hidden_dimension):\n",
    "        super(RegressionNetwork, self).__init__()\n",
    "\n",
    "        self.input_dimension = input_dimension\n",
    "        self.hidden_dimension = hidden_dimension\n",
    "\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Linear(in_features=self.input_dimension, out_features=self.hidden_dimension),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=self.hidden_dimension, out_features=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.layer_stack(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep it a little more general, we further implement another forward network class which can be used for regression and classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dimension, hidden_dimension, output_dimension, output_activation = \"identity\"):\n",
    "        \n",
    "        self.input_dimension = input_dimension\n",
    "        self.hidden_dimension = hidden_dimension\n",
    "        self.output_dimension = output_dimension\n",
    "        self.output_activation = output_activation\n",
    "\n",
    "        if not(self.output_activation in (\"identity\", \"binary\", \"multi\")):\n",
    "            raise NameError(\"Make sure that output activation is one of: identity, binary, multi\")\n",
    "        \n",
    "        if (self.output_activation in (\"identity\", \"binary\")) and (self.output_dimension > 1):\n",
    "            raise ValueError(\"If output activation is identity or binary, the output_dim argument must be set to 1.\")\n",
    "        \n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        if self.output_activation == \"identity\":\n",
    "            self.output_function = nn.Identity()\n",
    "        elif self.output_activation == \"binary\":\n",
    "            self.output_function = nn.Sigmoid()\n",
    "        elif self.output_activation == \"multi\":\n",
    "            self.output_function = nn.Softmax(dim = 1)\n",
    "\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Linear(in_features=self.input_dimension, out_features=self.hidden_dimension),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=self.hidden_dimension, out_features=self.output_dimension),\n",
    "            self.output_function\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.layer_stack(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The importance of gradient calculation and automatic differentiation\n",
    "\n",
    "Usually all neural network are trained with numerical optimization routines which use gradient information. Networks are calibrated by parameters. This is done be utilizing feature realizations to generate predictions under the current parameter setting and evaluate how much these predictions are in line with actual target realizations. The evaluation at this part is done by a loss function which is the lower, the more the prediction is in line with the target observation. Let $F$ denote the neural network and $\\Theta$ its parameters, then a prediction $\\hat{y}$ for input $\\boldsymbol{x}$ is generated by \n",
    "\n",
    "$$\n",
    "\\hat{y} = F_{\\Theta}\\left( \\boldsymbol{x} \\right)\n",
    "$$\n",
    "\n",
    "The loss function receives the true observation and the predicted value. Its value can only be changed by adjusting the parameters of the network. \n",
    "\n",
    "$$\n",
    "L\\left( \\Theta \\right) = \\sum_i l\\left(y_i, \\hat{y}_i\\right)\n",
    "$$\n",
    "\n",
    "The gradient $\\nabla_L$ of $L\\left( \\Theta \\right)$ includes all partial derivates of $L$ with respect to every parameter in the network. For every partial derivative a positive value indicates an increase in the loss function if the parameter is further raised while a negative value indicates a decrease in the loss function. As a decrease for the loss function is desired, one decreases the parameter value if the partial derivative is positive and increases its value if the partial derivative is negative. This rule can be subsumed by:\n",
    "\n",
    "$$\n",
    "\\Theta \\leftarrow \\Theta - \\eta \\nabla_L\n",
    "$$\n",
    "\n",
    "where $\\eta$ is the learning rate which controls the size of the parameter change. Note that the loss is a sum of individual loss values for each observation. It is no problem to determine the gradient for this as the derivative for the sum is the sum of derivatives. This makes $\\nabla_L$ a measure how the parameter change (positive or negative) alters predictive quality on average over these observations. Consequently, the parameter update improves predictions on average, but, not necessarily all of them. Furthermore, the gradient update is usually done by using only a subset of the data at each iteration. This is called batch gradient descent and counterbalances advantages and disadvantages of full gradient descent (using all data points for an update) and stochastic gradient descent (using only a single data point for an update).\n",
    "\n",
    "Overall, is is important to determine derivatives fast and accurate for arbitrary functions and their compositions. Pytorch determines derivatives automatically during calculation. This can be seen in a simple example below where we determine the derivative $\\partial f / \\partial x$ of the function $f(x)=x^2$. If we need the derivative, the *requires_grad* argument must be set to True for the tensor. The *backward* method determines the gradient automatically and the value of the gradient is determined by the *grad* attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The derivative of the function with respect to x at a value of: [2.] is equal to [4.]\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([2.], requires_grad=True)\n",
    "y = x**2\n",
    "y.backward()\n",
    "grad = x.grad\n",
    "print(f\"The derivative of the function with respect to x at a value of: {x.detach().numpy()} is equal to {x.grad.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate how we can use the gradient information, let us solve a simple example. The model is given by: $f_{\\theta}(x) = \\theta x$, thus it only depends on $\\theta$. For a datapoint $(2, 3)$, we want to find $\\theta$ which minimizes $L(\\theta) = \\left(y - \\theta x \\right)^2$. We start with a arbitrary value of $\\theta$ and repeat the gradient descent rule:\n",
    "\n",
    "* determine $ \\nabla_L = \\frac{\\partial L}{\\partial \\theta}$\n",
    "* update the current value of theta with: $\\theta \\leftarrow \\theta - \\eta \\nabla_L$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current value of theta: [0.09648111]\n",
      "Loss value: [7.8794613]\n",
      "Gradient value: [-11.228151]\n",
      "Current value of theta: [1.2192962]\n",
      "Loss value: [0.31517845]\n",
      "Gradient value: [-2.2456303]\n",
      "Current value of theta: [1.4438592]\n",
      "Loss value: [0.01260715]\n",
      "Gradient value: [-0.44912624]\n",
      "Current value of theta: [1.4887718]\n",
      "Loss value: [0.00050429]\n",
      "Gradient value: [-0.08982563]\n",
      "Current value of theta: [1.4977543]\n",
      "Loss value: [2.0172038e-05]\n",
      "Gradient value: [-0.01796532]\n",
      "Current value of theta: [1.4995508]\n",
      "Loss value: [8.0705286e-07]\n",
      "Gradient value: [-0.00359344]\n",
      "Current value of theta: [1.4999101]\n",
      "Loss value: [3.2316393e-08]\n",
      "Gradient value: [-0.00071907]\n",
      "Current value of theta: [1.499982]\n",
      "Loss value: [1.2960868e-09]\n",
      "Gradient value: [-0.000144]\n",
      "Current value of theta: [1.4999964]\n",
      "Loss value: [5.1159077e-11]\n",
      "Gradient value: [-2.861023e-05]\n",
      "Current value of theta: [1.4999993]\n",
      "Loss value: [2.046363e-12]\n",
      "Gradient value: [-5.722046e-06]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "eta = 0.1\n",
    "theta = torch.randn((1,), requires_grad=True)\n",
    "x = torch.tensor([2.])\n",
    "y = torch.tensor([3.])\n",
    "\n",
    "for _ in range(10):\n",
    "    # Forward pass\n",
    "    y_hat = theta * x\n",
    "    loss = (y - y_hat)**2\n",
    "\n",
    "    # Print current values\n",
    "    print(f\"Current value of theta: {theta.detach().numpy()}\")\n",
    "    print(f\"Loss value: {loss.detach().numpy()}\")\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    grad = theta.grad\n",
    "\n",
    "    # Print gradient\n",
    "    print(f\"Gradient value: {grad.detach().numpy()}\")\n",
    "\n",
    "    # Update parameter with no_grad to make sure this operation does not intrude gradient calculation\n",
    "    with torch.no_grad():\n",
    "        theta -= eta * grad\n",
    "\n",
    "    # Zero gradients (empty gradient to avoid accumulation over iterations)\n",
    "    theta.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how such procedures are usually handled with pytorch, we reproduce the example below with the usage of a linear layer, a pre-defined loss function and an instance of the stochastic gradient descent optimizer that takes care of parameter adjustments by gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta after gradient update: 1.2191189527511597\n",
      "Theta after gradient update: 1.4438238143920898\n",
      "Theta after gradient update: 1.488764762878418\n",
      "Theta after gradient update: 1.4977529048919678\n",
      "Theta after gradient update: 1.4995505809783936\n",
      "Theta after gradient update: 1.4999101161956787\n",
      "Theta after gradient update: 1.4999819993972778\n",
      "Theta after gradient update: 1.4999964237213135\n",
      "Theta after gradient update: 1.4999992847442627\n",
      "Theta after gradient update: 1.4999998807907104\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import SGD\n",
    "\n",
    "eta = 0.1\n",
    "f_theta = nn.Linear(in_features=1, out_features=1, bias = False)\n",
    "\n",
    "x = torch.tensor([2.])\n",
    "y = torch.tensor([3.])\n",
    "\n",
    "optimizer = SGD(f_theta.parameters(), lr = 0.1)\n",
    "loss_fun = nn.MSELoss()\n",
    "\n",
    "for _ in range(10):\n",
    "    y_hat = f_theta(x)\n",
    "    loss = loss_fun(y_hat, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    print(f\"Theta after gradient update: {f_theta.weight.detach().numpy().flatten()[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing with pytorch\n",
    "\n",
    "Usually, data is split at least into training and test data. For a neural network, usually parameters are updated after receiving gradient information for a batch of the overall training data set. This can be handled by combining the Dataset and DataLoader classes of pytorch. Dataset includes some toy datasets and the ability to define your own dataset. A self-defined dataset must include a method for initialization (*\\_\\_init__*), to determine the size of the data set (*\\_\\_len__*) and to retrieve an observation at a given index (*\\_\\_getitem__*). Below we first define a custom class for a dataset which works for almost every pandas dataframe. We retrieve the California Housing from sklearn, split it, standardize features and initialize it with our dataset class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class CaliforniaDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = torch.from_numpy(self.df.iloc[idx, :].values)\n",
    "        features, target = row[:-1], row[-1:]\n",
    "        features = features.type(torch.float32)\n",
    "        target = target.type(torch.float32)\n",
    "        return features, target\n",
    "\n",
    "cf_housing = fetch_california_housing()\n",
    "cf_df = pd.DataFrame(cf_housing.data, columns = cf_housing.feature_names)\n",
    "cf_df.loc[:, cf_housing.target_names] = cf_housing.target\n",
    "\n",
    "training_data_df, test_data_df = train_test_split(cf_df, train_size=0.7, shuffle=True, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_df, y_train_df = training_data_df.drop([\"MedHouseVal\"], axis = 1), training_data_df.loc[:, [\"MedHouseVal\"]]\n",
    "X_test_df, y_test_df = test_data_df.drop([\"MedHouseVal\"], axis = 1), test_data_df.loc[:, [\"MedHouseVal\"]]\n",
    "X_train_df_s = pd.DataFrame(scaler.fit_transform(X_train_df), index = training_data_df.index, columns = training_data_df.columns[:-1])\n",
    "X_test_df_s = pd.DataFrame(scaler.transform(X_test_df), index = test_data_df.index, columns = test_data_df.columns[:-1])\n",
    "train_df_s = pd.concat((X_train_df_s, y_train_df), axis = 1)\n",
    "test_df_s = pd.concat((X_test_df_s, y_test_df), axis = 1)\n",
    "\n",
    "training_data, test_data = CaliforniaDataset(train_df_s), CaliforniaDataset(test_df_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the pytorch Dataset is created, it can be handled by the DataLoader. Usually, we set the batch size and if data is supposed to be shuffled when creating the batches. The example demonstrates how to create instances of the Dataloader for training and test data. Training data comes in non-shuffled batches of size 64, while the test data is evaluated later at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1\n",
      "Feature batch size: torch.Size([64, 8])\n",
      "Target batch size: torch.Size([64, 1])\n",
      "Batch 2\n",
      "Feature batch size: torch.Size([64, 8])\n",
      "Target batch size: torch.Size([64, 1])\n",
      "Batch 3\n",
      "Feature batch size: torch.Size([64, 8])\n",
      "Target batch size: torch.Size([64, 1])\n",
      "Batch 4\n",
      "Feature batch size: torch.Size([64, 8])\n",
      "Target batch size: torch.Size([64, 1])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size = 64, shuffle=False)\n",
    "test_dataloader = DataLoader(test_data, batch_size = test_data_df.shape[0], shuffle=False)\n",
    "\n",
    "for i, (train_features, train_labels) in enumerate(train_dataloader):\n",
    "    print(f\"Batch {i+1}\")\n",
    "    print(f\"Feature batch size: {train_features.size()}\")\n",
    "    print(f\"Target batch size: {train_labels.size()}\")\n",
    "    if i > 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a neural network\n",
    "\n",
    "Now, let us put together all previous building blocks to train a neural network for the California Housing data set using pytorch. Below, we initialize the regression task network with ten hidden neurons, a stochastic gradient descent optimizer and the mean squared error loss function. Ove 20 epochs, we repeat to retrieve batches of size 64 from the training data, determine the gradients and use this information to update parameters. After all batches were used, one training epoch is finished and we print the average loss over all batches. Next with the *no_grad* method, we evaluate the loss for the full test data sample. This method makes sure that no gradient information from the test data is used for the training process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average training batch loss for the epoch 1 is: 1.4277\n",
      "The test loss after epoch 1 is: 0.6044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average training batch loss for the epoch 2 is: 0.5843\n",
      "The test loss after epoch 2 is: 0.5370\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average training batch loss for the epoch 3 is: 0.5227\n",
      "The test loss after epoch 3 is: 0.4944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average training batch loss for the epoch 4 is: 0.4896\n",
      "The test loss after epoch 4 is: 0.4734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average training batch loss for the epoch 5 is: 0.4732\n",
      "The test loss after epoch 5 is: 0.4611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average training batch loss for the epoch 6 is: 0.4636\n",
      "The test loss after epoch 6 is: 0.4532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average training batch loss for the epoch 7 is: 0.4566\n",
      "The test loss after epoch 7 is: 0.4467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average training batch loss for the epoch 8 is: 0.4504\n",
      "The test loss after epoch 8 is: 0.4406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average training batch loss for the epoch 9 is: 0.4447\n",
      "The test loss after epoch 9 is: 0.4346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average training batch loss for the epoch 10 is: 0.4398\n",
      "The test loss after epoch 10 is: 0.4293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average training batch loss for the epoch 11 is: 0.4351\n",
      "The test loss after epoch 11 is: 0.4245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average training batch loss for the epoch 12 is: 0.4301\n",
      "The test loss after epoch 12 is: 0.4202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average training batch loss for the epoch 13 is: 0.4259\n",
      "The test loss after epoch 13 is: 0.4162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average training batch loss for the epoch 14 is: 0.4218\n",
      "The test loss after epoch 14 is: 0.4123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average training batch loss for the epoch 15 is: 0.4179\n",
      "The test loss after epoch 15 is: 0.4088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average training batch loss for the epoch 16 is: 0.4143\n",
      "The test loss after epoch 16 is: 0.4052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average training batch loss for the epoch 17 is: 0.4109\n",
      "The test loss after epoch 17 is: 0.4020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average training batch loss for the epoch 18 is: 0.4078\n",
      "The test loss after epoch 18 is: 0.3993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average training batch loss for the epoch 19 is: 0.4052\n",
      "The test loss after epoch 19 is: 0.3970\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average training batch loss for the epoch 20 is: 0.4029\n",
      "The test loss after epoch 20 is: 0.3950\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import SGD\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "regression_network = RegressionNetwork(input_dimension=8, hidden_dimension=10)\n",
    "optimizer = SGD(regression_network.parameters(), lr = 0.01)\n",
    "loss_fun = nn.MSELoss()\n",
    "\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss, num_batches = 0.0, 0\n",
    "    for features, targets in train_dataloader:\n",
    "        outputs = regression_network(features)\n",
    "        loss = loss_fun(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        epoch_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    print(f\"The average training batch loss for the epoch {epoch+1} is: {epoch_loss/num_batches:.4f}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        test_features, test_targets = next(iter(test_dataloader))\n",
    "        test_outputs = regression_network(test_features)\n",
    "        test_loss = loss_fun(test_outputs, test_targets)\n",
    "        print(f\"The test loss after epoch {epoch + 1} is: {test_loss.item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}