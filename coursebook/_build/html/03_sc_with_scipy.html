
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Scientific computing with scipy &#8212; Scientific Computing and Digital Reporting with Python</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '03_sc_with_scipy';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Collecting data" href="04_getting_data.html" />
    <link rel="prev" title="Scientific computing with numpy" href="02_sc_with_numpy.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="00_welcome.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/course_logo.png" class="logo__image only-light" alt="Scientific Computing and Digital Reporting with Python - Home"/>
    <script>document.write(`<img src="_static/course_logo.png" class="logo__image only-dark" alt="Scientific Computing and Digital Reporting with Python - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="00_welcome.html">
                    Welcome
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="01_python_basics.html">How to use and install python</a></li>

<li class="toctree-l1"><a class="reference internal" href="02_sc_with_numpy.html">Scientific computing with numpy</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Scientific computing with scipy</a></li>
<li class="toctree-l1"><a class="reference internal" href="04_getting_data.html">Collecting data</a></li>
<li class="toctree-l1"><a class="reference internal" href="05_data.html">Working with Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="06_graphics.html">Graphics</a></li>
<li class="toctree-l1"><a class="reference internal" href="07_sklearn.html">SciKit Learn</a></li>
<li class="toctree-l1"><a class="reference internal" href="08_torch.html">Pytorch</a></li>

<li class="toctree-l1"><a class="reference internal" href="09_digital_reporting.html">Digital reporting</a></li>
<li class="toctree-l1"><a class="reference internal" href="A_math_statistics.html">Appendix - Math and statistics</a></li>



</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button"
   title="Source repository"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/03_sc_with_scipy.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Scientific computing with scipy</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#random-variables">Random variables</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#univariate-random-variables">Univariate random variables</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multivariate-random-variables">Multivariate random variables</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#probabilty-distributions">Probabilty distributions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optimization">Optimization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#one-dimensional-functions">One-dimensional functions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-dimensional-functions">Multi-dimensional functions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#important-characteristics-for-optimization">Important characteristics for optimization</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#convex-vs-non-convex-functions">Convex vs. non-convex functions</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#optimization-under-constraints">Optimization under constraints</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#absence-of-analytical-solutions">Absence of analytical solutions</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#examples">Examples</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#least-squares-minimization">Least-squares minimization</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#log-likelihood-maximization">Log-likelihood maximization</a></li>
</ul>
</li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span><span class="p">,</span> <span class="n">optimize</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pylab</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
</pre></div>
</div>
</div>
</div>
<section class="tex2jax_ignore mathjax_ignore" id="scientific-computing-with-scipy">
<h1>Scientific computing with scipy<a class="headerlink" href="#scientific-computing-with-scipy" title="Link to this heading">#</a></h1>
<p>Another powerful package which can be used for the application of mathematical algorithms as well as for statistics is the <a class="reference external" href="https://scipy.org/"><strong>scipy</strong></a> package. It is build upon numpy and can be used for many different methodical applications. We are going to focus on its <em>stats</em> and <em>optimize</em> module as both modules provide useful methods for the financial domain.</p>
<section id="random-variables">
<h2>Random variables<a class="headerlink" href="#random-variables" title="Link to this heading">#</a></h2>
<p>On financial markets, the majority of developments and linkages are non-deterministic and, thus, must be modeled assuming probability distributions. For instance, the market value development of a company captured by its returns, the default of a debtor, or the co-movement of returns from two or more companies. All these things are uncertain and can be modeled by random variables. Random variables are either <em>discrete</em> or <em>continuous</em>. The former has only a finite amount of realizations (or infinite countable realizations), while the latter can exhibit an infinite amount of different realizations. Furthermore, if we only examine a single random variable, we use a <em>univariate</em> distribution, while <em>multivariate</em> distributions are used for probability modeling of more then one variable. Multivariate modeling needs to deal with univariate characteristics such as the center of a distribution and the dependence between the random variables.</p>
<section id="univariate-random-variables">
<h3>Univariate random variables<a class="headerlink" href="#univariate-random-variables" title="Link to this heading">#</a></h3>
<p>For a discrete random variable, the function assigning probabilities to its realizations is called <strong>probability mass function</strong> and is given by a list-alike definition:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
f(x) = \begin{cases}
			P_X(\{ x \}) &amp; \text{for } X = x  \\
			0 &amp; \, \text{else}
         \end{cases}
\end{split}\]</div>
<p>The function</p>
<div class="math notranslate nohighlight">
\[
F(x) = P_X\left( \{X |X \leq x \} \right) = P_X\left( (-\infty, x] \right) = \sum_{x_i \leq x} f(x_i)
\]</div>
<p>is called <strong>cumulative distribution function</strong>.</p>
<p>For a continuous random variable, the probability of a concrete realization is equal to 0, <span class="math notranslate nohighlight">\(P(X = x) = 0\)</span>. Instead of directly defining a function mapping probabilities to subsets of <span class="math notranslate nohighlight">\(\mathbb{R}\)</span>, we define a <strong>probability density function</strong> <span class="math notranslate nohighlight">\(f(x)\)</span> which enables us to determine probabilities by integration. The domain of this function must be the set of all possibles states of <span class="math notranslate nohighlight">\(X\)</span>. Furthermore, it must hold the <span class="math notranslate nohighlight">\(f(x) \geq 0\)</span> for all <span class="math notranslate nohighlight">\(x\)</span> and that <span class="math notranslate nohighlight">\(\int_{-\infty}^{\infty} f(x)dx = 1\)</span>. With this definition the corresponding cumulative distribution function is given by:</p>
<div class="math notranslate nohighlight">
\[
F(x) = \int_{-\infty}^{x} f(t)dt
\]</div>
<p>Probability distributions can be compared by numbers which summarize their characteristics such as location, variation, shape and so on. For this purpose, ordinary and central moments of the distributions are used.</p>
<p>The <strong>expectation</strong> or <strong>expected value</strong> of a random variable if defined as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
E(X) = \begin{cases}
			\sum_i x_i \cdot f(x_i) &amp;  X   \text{ discrete} \\
			\int_{-\infty}^{\infty} x \cdot f(x) dx &amp; X \text{ continuous}
       \end{cases}
\end{split}\]</div>
<p>The <strong>variance</strong> measures how much realizations of a random variable vary and is defined by:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
Var(X) = E(X-E(X))^2 =
			\begin{cases}
			\sum_i (x_i - E(X))^2 \cdot f(x_i)   &amp; X \text{ discrete} \\
			\int_{-\infty}^{\infty} (x - E(X))^2 \cdot f(x) dx  &amp; X \text{ continuous}
			\end{cases}
\end{split}\]</div>
<p>For the variance, it holds that <span class="math notranslate nohighlight">\(Var(a + bX) = b^2 Var(X)\)</span>. The square root of the variance defines the <strong>standard deviation</strong> which is often denoted by the symbol <span class="math notranslate nohighlight">\(\sigma_X\)</span>.</p>
<p>If a probability distribution is not symmetric it is skewed. To quantify <strong>skewness</strong>, we use:</p>
<div class="math notranslate nohighlight">
\[
\frac{E \left[\left(X - E(X)\right)^3 \right]}{\sigma_X^3}  
\]</div>
<p>For negative values, the distribution is left skewed and for positive values, the distribution is right skewed. In addition of skewness, <strong>kurtosis</strong> is also often used for the characterization of probability distributions.</p>
<div class="math notranslate nohighlight">
\[
\frac{E \left[\left(X - E(X)\right)^4 \right]}{\sigma_X^4}  
\]</div>
<p>To characterize the level of kurtosis, the normal distribution is usually taken as a reference. It exhibits a kurtosis of <span class="math notranslate nohighlight">\(3\)</span>. If distributions exhibit higher kurtosis than <span class="math notranslate nohighlight">\(3\)</span>, we speak of excess kurtosis and leptokurtic distributions. These distributions are characterized by higher probability mass in the tails of the distribution. This means, extreme outcomes of the random variable are more likely. If a distribution exhibits a kurtosis lower then <span class="math notranslate nohighlight">\(3\)</span>, we say it is platokurtic.</p>
<p>Besides those four characteristic measures, quantiles of probability distributions can be used for informational purposes. Let us denote <span class="math notranslate nohighlight">\(F^{-1}: [0, 1] \to \mathbb{R}\)</span> as the inverse of the cumulative probability density function. Given some probability <span class="math notranslate nohighlight">\(\alpha\)</span>, we say <span class="math notranslate nohighlight">\(x_{\alpha}\)</span> is the <span class="math notranslate nohighlight">\(\alpha\)</span> <strong>quantile</strong> of <span class="math notranslate nohighlight">\(X\)</span>, if:</p>
<div class="math notranslate nohighlight">
\[
1 - F(x_{\alpha}) \geq 1 - \alpha ~~\text{and } F(x_{\alpha}) \geq \alpha
\]</div>
</section>
<section id="multivariate-random-variables">
<h3>Multivariate random variables<a class="headerlink" href="#multivariate-random-variables" title="Link to this heading">#</a></h3>
<p>Typically, we will take a look at multiple random variables at the same time. Very often certain relationships exist between these variables. The distribution over multiple random variables is described by their univariate characteristics together with their linkages. Given <span class="math notranslate nohighlight">\(p\)</span> random variables which are described by a random vector <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span>, the distribution is described by its joint distribution function</p>
<div class="math notranslate nohighlight">
\[
F(\boldsymbol{x}) = F(x_1, ..., x_p) = P(X_1 \leq x_1, ..., X_p \leq x_p)
\]</div>
<p>Technically <span class="math notranslate nohighlight">\(F\)</span> can be separated into <span class="math notranslate nohighlight">\(F(x_1, ..., x_f) = C(F_{X_1}(x_1), ..., F_{X_p}(x_p))\)</span> where <span class="math notranslate nohighlight">\(C\)</span> is called a copula which specifies the dependence structure between the univariate random variables and <span class="math notranslate nohighlight">\(F_{X_1}, ..., F_{X_p}\)</span> are univariate distributions.</p>
<p>Random variables <span class="math notranslate nohighlight">\(X_1, ..., X_p\)</span> are <strong>independent</strong> if their joint distribution is given by the product of their univariate distributions:</p>
<div class="math notranslate nohighlight">
\[
f(x_1, ...., x_p) = f_{X_1}(x_1) \cdot ... \cdot f_{X_p}(x_p)
\]</div>
<p>To quantify <strong>linear dependence</strong> between two random variables the pairwise <strong>covariance</strong> is used.</p>
<div class="math notranslate nohighlight">
\[
Cov(X,Y) = E\left[(X_1-E(X_1))\cdot (X_2-E(X_2)) \right] = E(X_1\cdot X_2) - E(X_1)E(X_2)
\]</div>
<p>The covariance can be scaled by the standard deviations of <span class="math notranslate nohighlight">\(X_1, X_2\)</span> which leads to the <strong>coefficient of linear correlation</strong> which is in the range <span class="math notranslate nohighlight">\([-1, 1]\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\rho = \frac{Cov(X_1,X_2)}{\sqrt{Var(X_1)} \cdot \sqrt{Var(X_2)}}
\]</div>
<p>It is important to understand that only linear dependence is captured adequately by covariance. Therefore, it is sometimes more reasonable to take into account metrics which also capture non-linear dependencies in an adequate way. An example is Spearmanâ€™s rho which is defined by:</p>
<div class="math notranslate nohighlight">
\[
\rho_S = \frac{Cov(rg_{X_1},rg_{X_2})}{ \sigma_{rg_{X_1}} \cdot \sigma_{rg_{X_2}}}
\]</div>
<p>where <span class="math notranslate nohighlight">\(rg\)</span> stands for the rank of the realization and not its value.</p>
</section>
<section id="probabilty-distributions">
<h3>Probabilty distributions<a class="headerlink" href="#probabilty-distributions" title="Link to this heading">#</a></h3>
<p>Random variables are usually modeled by parametric distributions which means that certain parameter values are used for calculating densities and, thus, determine the distribution. Once we assume a certain distribution and its parameters, we can do different things, e.g., determine moments for the distribution, determine probabilities for the outcomes of a random process, simulate data of a random process, etc. Scipy implements a variety of discrete and continuous probability distributions which can be found <a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/stats.html">here</a>. Each distribution comes with a variety of useful methods that can be found <a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.html#scipy.stats.rv_continuous">here</a>. Below, you can see a demonstration how to work with the <code class="docutils literal notranslate"><span class="pre">pdf</span></code>, <code class="docutils literal notranslate"><span class="pre">cdf</span></code>, <code class="docutils literal notranslate"><span class="pre">rvs</span></code> and <code class="docutils literal notranslate"><span class="pre">ppf</span></code> methods which implement the density, the cumulative density function, a way to simulate data for this distribution and the inverse cumulative density function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">size</span> <span class="o">=</span> <span class="mi">10000</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">density</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.75</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;simulated data&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;density function&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="n">u</span><span class="p">))</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;density function&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;cumulative density function&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;simulated data&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;inverse cumulative density function&quot;</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Standard normal distribution&quot;</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/73873380d4eeab50424ad391b7cca3975295f99b91da892d404006bcb1f2f98a.png" src="_images/73873380d4eeab50424ad391b7cca3975295f99b91da892d404006bcb1f2f98a.png" />
</div>
</div>
<p>Calculations can be done as you hopefully remember from your statistics classes. For instance, the probability <span class="math notranslate nohighlight">\(P \left(1 \leq X \leq 2 \right) = P \left( X \leq 2 \right) - P \left( X \leq 1 \right)\)</span> can be determined by:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">-</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>np.float64(0.13590512198327787)
</pre></div>
</div>
</div>
</div>
<p>Or the <span class="math notranslate nohighlight">\(0.95\)</span> quantile <span class="math notranslate nohighlight">\(F^{-1}\left(0.95\right)\)</span> is given by:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mf">0.95</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>np.float64(1.644853626951472)
</pre></div>
</div>
</div>
</div>
<p>Note that sampling random variables from a distribution can be quite powerful. The Glivenko-Cantelli theorem shows that an empirical distribution converges to the true distribution (almost surely) as the number of independent and identically observations from a distribution grows. This means, if we collect a large number of random samples from the distribution, we can be certain that the main characteristics from the distribution are captured by the empirical estimates of the samples. Let me give you an example why we sometimes need to simulate data instead of directly calculate outcomes.</p>
<p>Given two random variables <span class="math notranslate nohighlight">\(X_1, X_2\)</span>, we may be interested in the linear combination <span class="math notranslate nohighlight">\(Z = X_1 + X_2\)</span>. If <span class="math notranslate nohighlight">\(X_1, X_2\)</span> are normally distributed with mean <span class="math notranslate nohighlight">\(\mu_1, \mu_2\)</span> and variance <span class="math notranslate nohighlight">\(\sigma_1^2, \sigma_2^2\)</span>, <span class="math notranslate nohighlight">\(Z\)</span> is again normally distributed with mean <span class="math notranslate nohighlight">\(\mu_1 + \mu_2\)</span> and variance <span class="math notranslate nohighlight">\(\sigma_1^2 + \sigma_2^2\)</span>. The same does not hold if <span class="math notranslate nohighlight">\(X_1, X_2\)</span> are lognormal distributed. Assume an insurance company sells two contracts whose damages <span class="math notranslate nohighlight">\(L_1, L_2\)</span> are lognormal distributed. If <span class="math notranslate nohighlight">\(X_1, X_2\)</span> are normally distributed, <span class="math notranslate nohighlight">\(L_1 = e^{X_1}, L_2 = e^{X_2}\)</span> are lognormal distributed. Now, assume the insurance company wants to determine the <span class="math notranslate nohighlight">\(0.95\)</span> quantile of the sum of losses from both contracts <span class="math notranslate nohighlight">\(S = L_1 + L_2\)</span>. This can not be done be determined by a closed-form inverse cumulative distribution function of <span class="math notranslate nohighlight">\(S\)</span> as we do not know the distribution of <span class="math notranslate nohighlight">\(S\)</span>. However, if we sample a large number from <span class="math notranslate nohighlight">\(L_1, L_2\)</span> and calculate the corresponding sums, we get the empirical distribution of <span class="math notranslate nohighlight">\(S\)</span> and can use its quantile as a close approximation to the actual one. See the cell below how to do this.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># number of random samples</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">10000</span>

<span class="c1"># sample normal random variables</span>
<span class="n">x_1</span><span class="p">,</span> <span class="n">x_2</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">size</span> <span class="o">=</span> <span class="n">n</span><span class="p">),</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">size</span> <span class="o">=</span> <span class="n">n</span><span class="p">)</span>
<span class="c1"># transform to lognormal random variables</span>
<span class="n">l_1</span><span class="p">,</span> <span class="n">l_2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x_1</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x_2</span><span class="p">)</span>
<span class="c1"># determine the sum of both losses</span>
<span class="n">s</span> <span class="o">=</span> <span class="n">l_1</span> <span class="o">+</span> <span class="n">l_2</span>

<span class="c1"># visualize</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">l_1</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&quot;$l_1$&quot;</span><span class="p">,</span> <span class="n">bins</span> <span class="o">=</span> <span class="mi">50</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">l_2</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&quot;$l_2$&quot;</span><span class="p">,</span> <span class="n">bins</span> <span class="o">=</span> <span class="mi">50</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.75</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&quot;$s$&quot;</span><span class="p">,</span> <span class="n">bins</span> <span class="o">=</span> <span class="mi">50</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">),</span> <span class="n">color</span> <span class="o">=</span> <span class="s2">&quot;grey&quot;</span><span class="p">,</span> <span class="n">linestyle</span> <span class="o">=</span> <span class="s2">&quot;--&quot;</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&quot;$\hat</span><span class="si">{F}</span><span class="s2">_s^{-1}(0.95)$&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The estimated quantile of S is: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">s</span><span class="p">,</span><span class="w"> </span><span class="mf">0.95</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/ec3b6769d2675d1184e203e3a8752f2e003a507ce399ef32ca3dcf469fa1e3b1.png" src="_images/ec3b6769d2675d1184e203e3a8752f2e003a507ce399ef32ca3dcf469fa1e3b1.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The estimated quantile of S is: 8.9435
</pre></div>
</div>
</div>
</div>
<p>Note that we neglected to set parameters of the distributions so far. As stated before, the parameters define the distribution. Distributions differ in the number of parameters which define them. Scipy implements all distributions in their standardized versions and shifts and scales all distributions by the <code class="docutils literal notranslate"><span class="pre">loc</span></code>and <code class="docutils literal notranslate"><span class="pre">scale</span></code>values which can be set. This means, given the parameters for the standardized distribution <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> and its density <span class="math notranslate nohighlight">\(f(x | \boldsymbol{\theta})\)</span>, the density of a variable with the same distribution, but with location and scale parameters <span class="math notranslate nohighlight">\(l, s\)</span> is determined by:</p>
<div class="math notranslate nohighlight">
\[
g(y | \boldsymbol{\theta}, l, s) = f\left(\frac{y - l}{s} | \boldsymbol{\theta}\right) \frac{1}{s}
\]</div>
<p>Given, we assume a certain distributional form for a random variable and collect random variables <span class="math notranslate nohighlight">\((Y_1, ..., Y_n)\)</span>. A popular choice to estimate a set of reasonable parameters for the distribution is given by maximum likelihood estimation. Assuming independent and identically distributed (iid) random variables, the likelihood of the data is given by:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}(\boldsymbol{\theta}) = \prod_{i = 1}^{n} f(Y_i | \boldsymbol{\theta})
\]</div>
<p>Out of technical reasons, we usually maximize the log-likelihood:</p>
<div class="math notranslate nohighlight">
\[
\ln\left( \mathcal{L}(\boldsymbol{\theta}) \right) = \sum_{i = 1}^{n} \ln \left(f(Y_i | \boldsymbol{\theta})\right)
\]</div>
<p>The parameter is estimated by:</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\theta}_{ML} = \arg \max_{\boldsymbol{\theta}} \ln\left( \mathcal{L}(\boldsymbol{\theta}) \right)
\]</div>
<p>This is implemented by scipy using the <code class="docutils literal notranslate"><span class="pre">fit</span></code> method. However, we may want to do this on our own, after the next subchapter which takes a look at optimization in scipy.</p>
</section>
</section>
<section id="optimization">
<h2>Optimization<a class="headerlink" href="#optimization" title="Link to this heading">#</a></h2>
<p>Optimization is an important tool in the financial domain. A rather obvious example is given by portfolio optimization which aims to distribute capital among investments in the most profitable (risk-adjusted) way. However, beyond such obvious examples, in many cases, decisions in the financial domain, are build upon empirical analysis which use models that are adjusted to empirical data in the best possible way. To specify what the best possible way is, let us quickly recap important aspects of optimization.</p>
<section id="one-dimensional-functions">
<h3>One-dimensional functions<a class="headerlink" href="#one-dimensional-functions" title="Link to this heading">#</a></h3>
<p>Given a real-valued function <span class="math notranslate nohighlight">\(f: \mathbb{R} \to \mathbb{R}\)</span>, we say that <span class="math notranslate nohighlight">\(x_{*}\)</span> is a local minimum, if <span class="math notranslate nohighlight">\(f\left( x_{*} \right) \leq f(x)\)</span> for all <span class="math notranslate nohighlight">\(x \in [x_{*} - \epsilon, x_{*} + \epsilon]\)</span>. Basically this means in a certain range of x, no function value is smaller or equal to <span class="math notranslate nohighlight">\(f\left( x_{*} \right)\)</span>. A global minimum is the value <span class="math notranslate nohighlight">\(x_{**}\)</span> for which it holds that <span class="math notranslate nohighlight">\(f\left( x_{*} \right) \leq f(x)\)</span> for all <span class="math notranslate nohighlight">\(x \in \mathbb{R}\)</span>. Usually it is enough to examine a function for minima, because if we want to find maxima of a function, we can search for the minima of <span class="math notranslate nohighlight">\(-f\)</span> due to the min-max-duality. See below, that the value on the x-axis which minimizes <span class="math notranslate nohighlight">\(f\)</span> is the same which maximizes <span class="math notranslate nohighlight">\(-f\)</span>. This is what is meant by it.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">f</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">((</span><span class="n">x</span> <span class="o">-</span> <span class="mf">0.7</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
<span class="n">f</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;f&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">-</span><span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;-f&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;min-max-duality&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/c8deb287798eccc248de69f2ba0f57865d83089d73bf226725dfb35b3e04479c.png" src="_images/c8deb287798eccc248de69f2ba0f57865d83089d73bf226725dfb35b3e04479c.png" />
</div>
</div>
<p>In order to search for candidates of local minima, we search for solutions of:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial f}{\partial x} = 0
\]</div>
<p>These are called stationary points <span class="math notranslate nohighlight">\(x^{\circ}\)</span>. To examine if the stationary point is an extreme, we further investigate the second derivative. If</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial^2 f}{\partial^2 x^{\circ}} &gt; 0
\]</div>
<p>then a local minimum is found at <span class="math notranslate nohighlight">\(x^{\circ}\)</span>. If the domain of <span class="math notranslate nohighlight">\(f\)</span> is bounded, we may further want to investigate the bounds as potential candidates.</p>
</section>
<section id="multi-dimensional-functions">
<h3>Multi-dimensional functions<a class="headerlink" href="#multi-dimensional-functions" title="Link to this heading">#</a></h3>
<p>The more common scenario is that we need the minimum of a function of type <span class="math notranslate nohighlight">\(f: \mathbb{R}^p \to \mathbb{R}\)</span> which means the function depends on <span class="math notranslate nohighlight">\(p\)</span> input variables. To search for stationary points, we look for input values <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> which satisfy:</p>
<div class="math notranslate nohighlight">
\[ 
\nabla f (\boldsymbol{x}) = \boldsymbol{0}
\]</div>
<p>where <span class="math notranslate nohighlight">\( \nabla f \)</span> is the gradient of <span class="math notranslate nohighlight">\(f\)</span> and can be determined by its partial derivatives:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\nabla f = 
\begin{pmatrix}
    \frac{\partial f}{\partial x_1} \\
    \vdots \\
    \frac{\partial f}{\partial x_p} \\
\end{pmatrix}
\end{split}\]</div>
<p>Given a stationary point, we further need to examine the Hessian matrix:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{H} (\boldsymbol{x}) = 
\begin{pmatrix}
    \frac{\partial^2 f}{\partial^2 x_1} (\boldsymbol{x}) &amp; \frac{\partial^2 f}{\partial x_1 \partial x_2} (\boldsymbol{x}) &amp; ... &amp; \frac{\partial^2 f}{\partial x_1 \partial x_p} (\boldsymbol{x}) \\
    \frac{\partial^2 f}{\partial x_2 \partial x_1} (\boldsymbol{x}) &amp; \frac{\partial^2 f}{\partial^2 x_2} (\boldsymbol{x}) &amp; ... &amp; \frac{\partial^2 f}{\partial x_2 \partial x_p} (\boldsymbol{x})\\
    \vdots  &amp; \vdots &amp; &amp; \vdots \\
    \frac{\partial^2 f}{\partial x_p \partial x_1} (\boldsymbol{x}) &amp; \frac{\partial^2 f}{\partial x_p \partial x_2} (\boldsymbol{x}) &amp; ... &amp; \frac{\partial^2 f}{\partial^2 x_p } (\boldsymbol{x})\\
\end{pmatrix}
\end{split}\]</div>
<p>If the Hessian matrix is positive definite (all its eigenvalues are positive) at this point, we found a local minimum. After identifying all local minima of a function, we should be able to identify one or multiple global minima.</p>
</section>
<section id="important-characteristics-for-optimization">
<h3>Important characteristics for optimization<a class="headerlink" href="#important-characteristics-for-optimization" title="Link to this heading">#</a></h3>
<p>The presented way for optimization is the best and easiest possible scenario. It assumes that derivatives can be determined and the roots for stationary points can be determined analytically. Often, this may not hold and depending on some characteristics of <span class="math notranslate nohighlight">\(f\)</span> further complications may occur.</p>
<section id="convex-vs-non-convex-functions">
<h4>Convex vs. non-convex functions<a class="headerlink" href="#convex-vs-non-convex-functions" title="Link to this heading">#</a></h4>
<p>Given a function <span class="math notranslate nohighlight">\(f:\mathbb{R} \to \mathbb{R}\)</span>, then <span class="math notranslate nohighlight">\(f\)</span> is strictly convex, if for all <span class="math notranslate nohighlight">\(x, y \in \mathbb{R}, x \neq y\)</span> with <span class="math notranslate nohighlight">\(\lambda \in (0, 1)\)</span> it holds that:</p>
<div class="math notranslate nohighlight">
\[ f(\lambda x + (1 - \lambda) y) &lt; \lambda f(x) + (1 - \lambda) f(y) \]</div>
<p>If you take a look at the plots below, you will realize that whenever you draw a line between two points of the convex function, the function value of the combination of these points is smaller than the corresponding combination of function values. It can be shown that a local minimum for a strictly convex function is also the global minimum. Thus, looking for the global minimum of a strictly convex function is easy, because we only need to identify the stationary point and can stop. However, if a function is non-convex, we need to examine all stationary points with great attention. The latter becomes hard, if we approach situations for which no analytical solution of stationary points can be determined.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fc</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">-</span> <span class="mi">1</span>
<span class="n">fnc</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">**</span><span class="mi">4</span> <span class="o">+</span> <span class="n">x</span><span class="o">**</span><span class="mi">3</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">-</span> <span class="mi">2</span><span class="o">*</span><span class="n">x</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">fc</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">fnc</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Convex&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Non-Convex&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/192b38b873faea6d96964f4c253675ed22125ce7da81e7ef1928f85a9c1f5433.png" src="_images/192b38b873faea6d96964f4c253675ed22125ce7da81e7ef1928f85a9c1f5433.png" />
</div>
</div>
</section>
<section id="optimization-under-constraints">
<h4>Optimization under constraints<a class="headerlink" href="#optimization-under-constraints" title="Link to this heading">#</a></h4>
<p>Sometimes the search space for minima is restricted by constraints of the form <span class="math notranslate nohighlight">\(c(x) \leq b\)</span>. Constraints can be included in the optimization process by different techniques such as substitution, Lagrange multipliers or by altering the objective function.</p>
</section>
<section id="absence-of-analytical-solutions">
<h4>Absence of analytical solutions<a class="headerlink" href="#absence-of-analytical-solutions" title="Link to this heading">#</a></h4>
<p>Sometimes stationary points can not be tracked analytically. Furthermore, determining the gradient itself may not be possible and its value must be approximated. For all these scenarios, approximation algorithm exists, however, these come along with deviations from true solutions and induce errors. This further complicates the search for a true local minimum.</p>
<p>Scipy implements many different methods algorithms for function optimization. These are all algorithms which determine the possible minima by search based algorithms, thus, approximating the true solutions. Some of them enable the user to provide the gradient and Hessian, respectively, or both can be approximated by different methods. Let us take a look at some examples how to work with scipy to optimize some functions.</p>
</section>
</section>
<section id="examples">
<h3>Examples<a class="headerlink" href="#examples" title="Link to this heading">#</a></h3>
<p>Let us take a look at some examples which are used in the financial domain, among others.</p>
<section id="least-squares-minimization">
<h4>Least-squares minimization<a class="headerlink" href="#least-squares-minimization" title="Link to this heading">#</a></h4>
<p>One popular example is least squares minimization. For instance assume, we have sample data: <span class="math notranslate nohighlight">\(\lbrace 1, 2, 3 \rbrace\)</span> and we want to determine <span class="math notranslate nohighlight">\(\mu\)</span> such that:</p>
<div class="math notranslate nohighlight">
\[
\min_{\mu} \sum_{i=1}^n \left( x_i - \mu \right)^2
\]</div>
<p>Thus, our function depends on a single variable <span class="math notranslate nohighlight">\(\mu\)</span> and outputs a single real valued number, i.e., <span class="math notranslate nohighlight">\(f: \mathbb{R} \to \mathbb{R}\)</span> with <span class="math notranslate nohighlight">\( f\left( \mu \right) = \sum_{i=1}^{n} \left(x_i - \mu\right)^2\)</span>. We can determine the analytical solution first. To find stationary points, we must find <span class="math notranslate nohighlight">\(\mu\)</span> such that:</p>
<div class="math notranslate nohighlight">
\[ \frac{\partial f}{\partial \mu} =   \sum_{i=1}^{n} -2 \left(x_i - \mu\right) \stackrel{!}{=} 0\]</div>
<p>This gives us:</p>
<div class="math notranslate nohighlight">
\[\begin{split} 
2 \sum_{i=1}^{n} \mu  = 2 \sum_{i=1}^{n} x_i \\
n \mu  = \sum_{i=1}^{n} x_i \\
\mu = \frac{1}{n} \sum_{i=1}^{n} x_i \\
\end{split}\]</div>
<p>as the potential candidate. Checking the value of the second derivative at this point <span class="math notranslate nohighlight">\(\frac{\partial^2 f}{\partial^2 \mu} \left( \mu \right) =  2 n &gt; 0\)</span> tells us that we found a local minimum which is also the global minimum. The plot visualizes the search for the minimum, on the x-axis you see potential candidates for <span class="math notranslate nohighlight">\(\mu\)</span> and corresponding values for <span class="math notranslate nohighlight">\(f(\mu)\)</span> can be found on the y-axis.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="n">sos</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">mu</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">x</span> <span class="o">-</span> <span class="n">mu</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
<span class="n">mu_candidates</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">mu_candidates</span><span class="p">,</span> <span class="p">[</span><span class="n">sos</span><span class="p">(</span><span class="n">mu</span><span class="p">)</span> <span class="k">for</span> <span class="n">mu</span> <span class="ow">in</span> <span class="n">mu_candidates</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\mu$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\sum_i \left(x_i - \mu\right)^2$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/83ddf7786528424fc8941ccf9f578088260f52cbc25f894348f95853093b1bc7.png" src="_images/83ddf7786528424fc8941ccf9f578088260f52cbc25f894348f95853093b1bc7.png" />
</div>
</div>
<p>For one-dimensional optimization problems, the <code class="docutils literal notranslate"><span class="pre">minimize_scalar</span></code>method of the optimize module can be used. It offers three different algorithms (brent, bounded, golden) to search for the minimum. As the name suggests, the bounded method can be used if we face a bounded optimization problem, i.e., we face some constraints regarding the search space for the function input. All arguments can be found <a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize_scalar.html#scipy.optimize.minimize_scalar">here</a>. Below you can see the output for our optimization problem. Besides a message which tells us, if the algorithm terminated successfully, <em>fun</em> informs us about the function value at the minimum which is given by <em>x</em>, <em>nit</em> is the number of iterations the algorithm took. You can see, we found a solution which is almost equal to the analytical solution after <span class="math notranslate nohighlight">\(5\)</span> iterations.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">res</span> <span class="o">=</span> <span class="n">optimize</span><span class="o">.</span><span class="n">minimize_scalar</span><span class="p">(</span><span class="n">sos</span><span class="p">,</span> <span class="n">bracket</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>
<span class="n">res</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> message: 
          Optimization terminated successfully;
          The returned value satisfies the termination criteria
          (using xtol = 1.48e-08 )
 success: True
     fun: 2.0
       x: 1.999999999999999
     nit: 5
    nfev: 8
</pre></div>
</div>
</div>
</div>
</section>
<section id="log-likelihood-maximization">
<h4>Log-likelihood maximization<a class="headerlink" href="#log-likelihood-maximization" title="Link to this heading">#</a></h4>
<p>As stated in the subchapter before, we can use maximum likelihood estimation to estimate the parameters of a probability distribution for some observed data <span class="math notranslate nohighlight">\( \lbrace x_1, ..., x_n \rbrace \)</span>. A popular distribution is the normal distribution. Even though, it often does not adequately capture real-world dynamics, it comes along with appealing mathematical and statistical properties. This is why it is sometime used even though it may come along with an oversimplification. Its negative log-likelihood depends on two parameters <span class="math notranslate nohighlight">\(\mu, \sigma^2\)</span> which also are the mean and variance of the distribution. For data <span class="math notranslate nohighlight">\( \lbrace x_1, ..., x_n \rbrace \)</span>, it is given by:</p>
<div class="math notranslate nohighlight">
\[
l \left( \mu, \sigma^2 \right) = - \frac{n}{2} \log \left(2 \pi \sigma^2 \right) - \frac{1}{2 \sigma^2} \sum_{i = 1}^n \left(x_i - \mu\right)^2
\]</div>
<p>Its gradient is given by:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\nabla l = 
\begin{pmatrix}
\frac{\partial l}{\partial \mu} \\
\frac{\partial l}{\partial \sigma^2} \\
\end{pmatrix}
=
\begin{pmatrix}
-\frac{1}{\sigma^2} \sum_{i = 1}^n \left(x_i - \mu\right) \\
\frac{1}{2\sigma^2} \left(n - \frac{1}{\sigma^2} \sum_{i = 1}^n \left(x_i - \mu\right)^2 \right)
\end{pmatrix}
\end{split}\]</div>
<p>Solving for the roots of both partial derivatives gives us the estimators:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mu = \frac{1}{n} \sum_{i = 1}^n x_i \\
\sigma^2 = \frac{1}{n} \sum_{i = 1}^n \left( x_i - \mu \right)^2
\end{split}\]</div>
<p>Let us skip the Hessian and take a look how we can derive the parameter estimates with the <code class="docutils literal notranslate"><span class="pre">minimze</span></code> method of the optimize module which is able to minimize functions with multiple input variables. Depending on the method, we can use the gradient for the minimization or not. If we have access to the gradient we may want to use the BFGS or L-BFGS-B method. If not, Nelder-Mead or Powell may be good methods to start. Below you can see that the analytical solutions are more or less the same as the solutions found by the L-BFGS-B as well as the Nelder-Mead methods. Note, that for more realistic scenarios we are not able to derive the analytical solution and need to rely upon solutions found by the <code class="docutils literal notranslate"><span class="pre">minimize</span></code> method, only. Further note, how simply we can include bounds for the parameters. Especially, for the variance, it is necessary to work with bounds as it must be a positive number. In practice, it might be a good idea to set some bounds for <span class="math notranslate nohighlight">\(\mu\)</span> as well and enlarge them if found solutions are at the edges of the bounds.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">neg_ll</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">mu</span><span class="p">,</span> <span class="n">sigma_sq</span> <span class="o">=</span> <span class="n">theta</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">ll</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="n">n</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">*</span><span class="n">sigma_sq</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">sigma_sq</span><span class="p">)</span><span class="o">**</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">x</span> <span class="o">-</span> <span class="n">mu</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">ll</span>

<span class="k">def</span><span class="w"> </span><span class="nf">grad</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">mu</span><span class="p">,</span> <span class="n">sigma_sq</span> <span class="o">=</span> <span class="n">theta</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">deriv_mu</span> <span class="o">=</span> <span class="o">-</span><span class="n">sigma_sq</span><span class="o">**</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">mu</span><span class="p">)</span>
    <span class="n">deriv_sigma</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">sigma_sq</span><span class="p">)</span><span class="o">**</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="n">sigma_sq</span><span class="o">**</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">x</span> <span class="o">-</span> <span class="n">mu</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">deriv_mu</span><span class="p">,</span> <span class="n">deriv_sigma</span>

<span class="c1"># sample some data of a standard normal distribution</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">size</span> <span class="o">=</span> <span class="mi">100</span><span class="p">)</span>

<span class="c1"># for comparison let us determine the analytical solutions of the mle</span>
<span class="n">mu_mle</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">sigma_sq_mle</span> <span class="o">=</span> <span class="p">((</span><span class="n">x</span><span class="o">-</span><span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">fr</span><span class="s2">&quot;MLE estimates for mu and sigma_squared are: </span><span class="si">{</span><span class="n">mu_mle</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">sigma_sq_mle</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># minimize with gradients and the L-BFGS-B method</span>
<span class="n">optimize</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">neg_ll</span><span class="p">,</span> <span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">]),</span> <span class="n">args</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">jac</span> <span class="o">=</span> <span class="n">grad</span><span class="p">,</span> <span class="n">method</span> <span class="o">=</span> <span class="s2">&quot;L-BFGS-B&quot;</span><span class="p">,</span> <span class="n">bounds</span> <span class="o">=</span> <span class="p">[(</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">x</span><span class="p">)),</span> <span class="p">(</span><span class="mf">0.001</span><span class="p">,</span> <span class="mi">5000</span><span class="p">)])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>MLE estimates for mu and sigma_squared are: -0.1038, 0.8165
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  message: CONVERGENCE: RELATIVE REDUCTION OF F &lt;= FACTR*EPSMCH
  success: True
   status: 0
      fun: 131.75879410959755
        x: [-1.038e-01  8.165e-01]
      nit: 7
      jac: [ 8.247e-07 -1.189e-05]
     nfev: 16
     njev: 16
 hess_inv: &lt;2x2 LbfgsInvHessProduct with dtype=float64&gt;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Optimization using the Nelder-Mead method</span>
<span class="n">optimize</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">neg_ll</span><span class="p">,</span> <span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">]),</span> <span class="n">args</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">method</span> <span class="o">=</span> <span class="s2">&quot;Nelder-Mead&quot;</span><span class="p">,</span> <span class="n">bounds</span> <span class="o">=</span> <span class="p">[(</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">x</span><span class="p">)),</span> <span class="p">(</span><span class="mf">0.001</span><span class="p">,</span> <span class="mi">5000</span><span class="p">)])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>       message: Optimization terminated successfully.
       success: True
        status: 0
           fun: 131.75879418567382
             x: [-1.038e-01  8.165e-01]
           nit: 42
          nfev: 83
 final_simplex: (array([[-1.038e-01,  8.165e-01],
                       [-1.038e-01,  8.166e-01],
                       [-1.039e-01,  8.165e-01]]), array([ 1.318e+02,  1.318e+02,  1.318e+02]))
</pre></div>
</div>
</div>
</div>
<p>We end this chapter here and leave other examples for the tutorial. Note, that many optimization routines such as maximum likelihood estimation for probability distributions usually are well implemented and we do not need to conduct optimization on our own. However, the more advanced models become, the higher the more likely it gets that we are in need of manually estimating models by optimization techniques. The <code class="docutils literal notranslate"><span class="pre">minimize</span></code> method of the optimize module is a good starting point, especially, for models whose parameters are bounded.</p>
</section>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="02_sc_with_numpy.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Scientific computing with numpy</p>
      </div>
    </a>
    <a class="right-next"
       href="04_getting_data.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Collecting data</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#random-variables">Random variables</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#univariate-random-variables">Univariate random variables</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multivariate-random-variables">Multivariate random variables</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#probabilty-distributions">Probabilty distributions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optimization">Optimization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#one-dimensional-functions">One-dimensional functions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-dimensional-functions">Multi-dimensional functions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#important-characteristics-for-optimization">Important characteristics for optimization</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#convex-vs-non-convex-functions">Convex vs. non-convex functions</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#optimization-under-constraints">Optimization under constraints</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#absence-of-analytical-solutions">Absence of analytical solutions</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#examples">Examples</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#least-squares-minimization">Least-squares minimization</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#log-likelihood-maximization">Log-likelihood maximization</a></li>
</ul>
</li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Prof. Dr. Ralf Kellner
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      Â© Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>