{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scientific computing with numpy\n",
    "\n",
    "[**Numpy**](https://numpy.org/doc/stable/) is a fundamental python package for scientific computing. It comes along with a flexible array definition and includes many important mathematical functions and routines. We start with the discussion of numpy because other fundamental libraries such as scipy or pandas use its ndarrays as a common format. Numpy can be intalled by ```pip install numpy``` and it is common to import numpy using the alias *np*.\n",
    "\n",
    "## Numpy fundamentals\n",
    "\n",
    "Numpy arrays can be created from other python sequences such as lists or tuples or they can be created by several functions. Arrays can have multiple dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One dimensional array from a list: \n",
      "[1 2 3]\n",
      "\n",
      "Two dimensional array from a list: \n",
      "[[1 2]\n",
      " [3 4]]\n",
      "\n",
      "Three dimensional array from a list: \n",
      "[[[1 2]\n",
      "  [3 4]]\n",
      "\n",
      " [[4 5]\n",
      "  [6 7]]]\n",
      "\n",
      "One dimensional array created with linspace: \n",
      "[ 1.   1.5  2.   2.5  3.   3.5  4.   4.5  5.   5.5  6.   6.5  7.   7.5\n",
      "  8.   8.5  9.   9.5 10. ]\n",
      "\n",
      "One dimensional array created with arange: \n",
      "[1.  1.5 2.  2.5 3.  3.5 4.  4.5 5.  5.5 6.  6.5 7.  7.5 8.  8.5 9.  9.5]\n",
      "\n",
      "Create an identity matrix: \n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "\n",
      "Create a matrix with ones: \n",
      "[[1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]]\n",
      "\n",
      "Create a matrix with zeros: \n",
      "[[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"One dimensional array from a list: \\n{np.array([1, 2, 3])}\\n\")\n",
    "print(f\"Two dimensional array from a list: \\n{np.array([[1, 2], [3, 4]])}\\n\")\n",
    "print(f\"Three dimensional array from a list: \\n{np.array([[[1, 2], [3, 4]], [[4, 5], [6, 7]]])}\\n\")\n",
    "print(f\"One dimensional array created with linspace: \\n{np.linspace(start = 1, stop = 10, num = 19)}\\n\")\n",
    "print(f\"One dimensional array created with arange: \\n{np.arange(1, 10, 0.5)}\\n\")\n",
    "print(f\"Create an identity matrix: \\n{np.eye(3)}\\n\")\n",
    "print(f\"Create a matrix with ones: \\n{np.ones((3, 2))}\\n\")\n",
    "print(f\"Create a matrix with zeros: \\n{np.zeros((3, 2))}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arrays can be indexed with the ```x[obj]``` syntax where x is the array and obj the selection. Index counting starts at $0$, single elements can be accessed with their index values. Negative indices start from the end of an array. If multiple values should be selected, we can use slicing which defines the selection by ```start:stop:step``` definitions. See below for some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-dimensional array x: \n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "\n",
      "Two-dimensional array X: \n",
      "[[0 1 2 3 4]\n",
      " [5 6 7 8 9]]\n",
      "\n",
      "Select the third element of x by x[2]: \n",
      "2\n",
      "\n",
      "Select the third element of the second row of  X by X[1, 2]: \n",
      "7\n",
      "\n",
      "Select the first three elements of x by x[:3]: \n",
      "[0 1 2]\n",
      "\n",
      "Select elements 4 to 6 of x by x[3:6]: \n",
      "[3 4 5]\n",
      "\n",
      "Select the last two elements of x by x[-2:]: \n",
      "[8 9]\n",
      "\n",
      "Select every second element of x by x[::2]: \n",
      "[0 2 4 6 8]\n",
      "\n",
      "Select the second column of X by X[:, 1]: \n",
      "[1 6]\n",
      "\n",
      "Select the first row of X by X[0, :]: \n",
      "[0 1 2 3 4]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "one_dim_array = np.arange(10)\n",
    "two_dim_array = np.reshape(one_dim_array, (2, 5))\n",
    "\n",
    "print(f\"One-dimensional array x: \\n{one_dim_array}\\n\")\n",
    "print(f\"Two-dimensional array X: \\n{two_dim_array}\\n\")\n",
    "print(f\"Select the third element of x by x[2]: \\n{one_dim_array[2]}\\n\")\n",
    "print(f\"Select the third element of the second row of  X by X[1, 2]: \\n{two_dim_array[1, 2]}\\n\")\n",
    "print(f\"Select the first three elements of x by x[:3]: \\n{one_dim_array[:3]}\\n\")\n",
    "print(f\"Select elements 4 to 6 of x by x[3:6]: \\n{one_dim_array[3:6]}\\n\")\n",
    "print(f\"Select the last two elements of x by x[-2:]: \\n{one_dim_array[-2:]}\\n\")\n",
    "print(f\"Select every second element of x by x[::2]: \\n{one_dim_array[::2]}\\n\")\n",
    "print(f\"Select the second column of X by X[:, 1]: \\n{two_dim_array[:, 1]}\\n\")\n",
    "print(f\"Select the first row of X by X[0, :]: \\n{two_dim_array[0, :]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When new objects are defined by selecting parts of existing arrays, be aware of the difference between a *view* and a *copy*. While an object created as a view shares data of the original array and is changed with it, the objected created as a copy won't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original array: [0 1 2 3 4 5 6 7 8 9]\n",
      "A new array as a view on x: [0 1]\n",
      "The original array after its change: [11 12  2  3  4  5  6  7  8  9]\n",
      "The array created out of x: [11 12]\n"
     ]
    }
   ],
   "source": [
    "# Create an object \n",
    "x = np.arange(10)\n",
    "# define a new object as a slice of x: note view is added hear for didactic purposes, the same happens without calling .view explicitly\n",
    "y = x[:2].view()\n",
    "print(f\"The original array: {x}\")\n",
    "print(f\"A new array as a view on x: {y}\")\n",
    "x[:2] = [11, 12]\n",
    "print(f\"The original array after its change: {x}\")\n",
    "print(f\"The array created out of x: {y}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original array: [0 1 2 3 4 5 6 7 8 9]\n",
      "A new array as a copy on x: [0 1]\n",
      "The original array after its change: [11 12  2  3  4  5  6  7  8  9]\n",
      "The array created out of x: [0 1]\n"
     ]
    }
   ],
   "source": [
    "# Create an object \n",
    "x = np.arange(10)\n",
    "# define a new object as a slice of x and copy it: note .copy is equivalent to so called advanced indexing which would be done by: y = x[[0, 1, 2]]\n",
    "y = x[:2].copy()\n",
    "print(f\"The original array: {x}\")\n",
    "print(f\"A new array as a copy on x: {y}\")\n",
    "x[:2] = [11, 12]\n",
    "print(f\"The original array after its change: {x}\")\n",
    "print(f\"The array created out of x: {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical operations with numpy\n",
    "\n",
    "Numpy arrays can be used for mathematical calculations which are vectorized. By broadcasting numpy often is able to figure out by itself how to handle mathematical operations for arrays with different shape. Let us take a look at some basic operations:\n",
    "\n",
    "* addition and multiplication of a scalar with a vector (one-dimensional array) or matrix (two-dimensional array) for each element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s: 2\n",
      "\n",
      "x: [1 2 3]\n",
      "\n",
      "X: \n",
      "[[1 2]\n",
      " [3 4]]\n",
      "\n",
      "Results for addition:\n",
      "[3 4 5]\n",
      "[[3 4]\n",
      " [5 6]]\n",
      "\n",
      "Results for multiplication:\n",
      "[2 4 6]\n",
      "[[2 4]\n",
      " [6 8]]\n"
     ]
    }
   ],
   "source": [
    "# scalar\n",
    "s = 2\n",
    "# vector\n",
    "x = np.array([1, 2, 3])\n",
    "# matrix\n",
    "X = np.array([[1, 2], [3, 4]])\n",
    "\n",
    "print(f\"s: {s}\\n\")\n",
    "print(f\"x: {x}\\n\")\n",
    "print(f\"X: \\n{X}\\n\")\n",
    "print(\"Results for addition:\")\n",
    "print(f\"{s + x}\")\n",
    "print(f\"{s + X}\\n\")\n",
    "print(\"Results for multiplication:\")\n",
    "print(f\"{s * x}\")\n",
    "print(f\"{s * X}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* addition and multiplication of vectors and matrices for each element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y: [4 5 6]\n",
      "\n",
      "Y: \n",
      "[[5 6]\n",
      " [7 8]]\n",
      "\n",
      "Results for vector addition and multiplication for each element:\n",
      "x + y: [5 7 9]\n",
      "x * y: [ 4 10 18]\n",
      "\n",
      "Results for matrix addition and multiplication for each element:\n",
      "X + Y: \n",
      "[[ 6  8]\n",
      " [10 12]]\n",
      "\n",
      "X * Y: \n",
      "[[ 5 12]\n",
      " [21 32]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y = np.array([4, 5, 6])\n",
    "Y = np.array([[5, 6], [7, 8]])\n",
    "\n",
    "print(f\"y: {y}\\n\")\n",
    "print(f\"Y: \\n{Y}\\n\")\n",
    "\n",
    "print(\"Results for vector addition and multiplication for each element:\")\n",
    "print(f\"x + y: {x + y}\")\n",
    "print(f\"x * y: {x * y}\\n\")\n",
    "print(\"Results for matrix addition and multiplication for each element:\")\n",
    "print(f\"X + Y: \\n{X + Y}\\n\")\n",
    "print(f\"X * Y: \\n{X * Y}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* matrix multiplication can be done by the ```np.matmul``` method or by the ```dot``` method of numpy arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dot product of x and y: 32\n",
      "Matrix multiplication of X and Y: \n",
      "[[19 22]\n",
      " [43 50]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dot product of x and y: {x.dot(y)}\")\n",
    "print(f\"Matrix multiplication of X and Y: \\n{np.matmul(X, Y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides basic operations, numpy comes along with many common functions from analysis and statistics. An overview can be found [here](https://numpy.org/doc/stable/reference/routines.math.html) and [here](https://numpy.org/doc/stable/reference/routines.statistics.html). We demonstrate the usage with a practical application to stock market data in the next subsection.\n",
    "\n",
    "## Fundamental stock market calculations with numpy\n",
    "\n",
    "In the cell below you can see how we import a dataset (we talk later about pandas datasets) which includes adjusted end of day stock prices for ten large US companies. The output shows you the stock prices for the first two days already in numpy array format. Note, that all we do here serves demonstrational purposes and we do this later in a more convenient way. Overall, the array contains 536 observations, thus, approximately two years of daily data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the array (number of observations, number of companies): (536, 10)\n",
      "\n",
      "Observations for the first two days:\n",
      "\n",
      "[[182.00999451 334.75       144.99150085 399.92666626 338.54000854\n",
      "  170.40449524 301.20999146 663.32000732 221.07032776  62.84332275]\n",
      " [179.69999695 329.01000977 144.39950562 383.19665527 336.52999878\n",
      "  167.52200317 292.8999939  670.91998291 222.09866333  65.20711517]]\n"
     ]
    }
   ],
   "source": [
    "close_prices = pd.read_csv(\"../data/close_prices_small.csv\", index_col = \"date\")\n",
    "stock_prices = close_prices.values\n",
    "print(f\"Shape of the array (number of observations, number of companies): {stock_prices.shape}\\n\")\n",
    "print(\"Observations for the first two days:\\n\")\n",
    "print(stock_prices[:2, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common way to analyze the development of companies on stock markets is by examining their returns. Discrete returns are defined by price ($s$) change between two periods $t-1$ and $t$ in relation to the price at $t-1$\n",
    "\n",
    "$$\n",
    "r_t = \\frac{s_t - s_{t-1}}{s_{t-1}} = \\frac{s_t}{s_{t-1}} - 1\n",
    "$$\n",
    "\n",
    "An alternative is given by logarithmic returns which assume continuous price development and are defined by:\n",
    "\n",
    "$$\n",
    "d_t = \\ln \\left( \\frac{s_t}{s_{t-1}} \\right)\n",
    "$$\n",
    "\n",
    "For small price changes, discrete and log-returns are very similar. Both have different mathematical properties. While discrete returns are additive in the cross-section (at the same point in time), log-returns are additive over time (over time). Basically, this means, if we want to determine an average return at the same point in time, e.g., a portfolio return of an investment into multiple assets, the average discrete return can be easily calculated by:\n",
    "\n",
    "$$\n",
    "r_t^{p} = \\frac{1}{n} \\sum_{i = 1}^n r_{t,i}\n",
    "$$\n",
    "\n",
    "The same is not possible for log-returns, however, the value development of an initial investment $p_0$ over time steps $t = 1, 2, 3, ..., T$ can be determined by:\n",
    "\n",
    "$$\n",
    "p_T = p_0 e^{\\sum_{t = 1}^{T} d_t}\n",
    "$$\n",
    "\n",
    "Luckily, we can transform discrete returns to log-returns and vice versa by:\n",
    "\n",
    "$$\n",
    "d_t = \\ln \\left( 1 + r_t \\right) \\\\\n",
    "r_t = e^{d_t} - 1\n",
    "$$\n",
    "\n",
    "Now, let us do the following:\n",
    "\n",
    "1. Calculate daily discrete returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete returns on the first day.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.0126916 , -0.0171471 , -0.00408297, -0.0418327 , -0.00593729,\n",
       "       -0.01691559, -0.02758872,  0.01145748,  0.00465162,  0.03761406])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discrete_returns = stock_prices[1:, :] / stock_prices[:-1, :] - 1\n",
    "print(\"Discrete returns on the first day.\")\n",
    "discrete_returns[0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get a feeling for the profit and risk of each company we determine the average discrete returns per company and estimate the standard deviation.\n",
    "Hereby, get to know the ```axis``` argument. It defines along which dimension the operation should be done. By default, it conducts the operation over all elements in the array. However, in our scenario, we want to conduct each operation per column which refers to the first dimension of the array and which is done by setting ```axis = 0```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average discrete returns per company: \n",
      "[ 0.00016443  0.00052218  0.00020384 -0.00063917  0.0012004   0.00032709\n",
      "  0.00211155  0.00138704  0.00053273  0.00113565]\n",
      "\n",
      "Estimated standard deviation for discrete returns per company: \n",
      "[0.01799022 0.0189567  0.02170079 0.03744394 0.0342092  0.02638379\n",
      " 0.03504752 0.02184552 0.01507697 0.0188964 ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average discrete returns per company: \\n{discrete_returns.mean(axis = 0)}\\n\")\n",
    "print(f\"Estimated standard deviation for discrete returns per company: \\n{discrete_returns.std(ddof = 1, axis = 0)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides individual metrics, a very important aspect on financial markets is how similar company returns behave over time. To calculate the (Bravais-Pearson) matrix, we can use the ```corrcoef``` method as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.73390036, 0.69870792, 0.55934367, 0.5480876 ,\n",
       "        0.61345432, 0.65443752, 0.64198519, 0.64278306, 0.20997612],\n",
       "       [0.73390036, 1.        , 0.72875756, 0.46279101, 0.59162586,\n",
       "        0.69162914, 0.69906846, 0.61477863, 0.56881728, 0.10738494],\n",
       "       [0.69870792, 0.72875756, 1.        , 0.44880208, 0.62986847,\n",
       "        0.6764548 , 0.63009485, 0.55986681, 0.50112936, 0.1148427 ],\n",
       "       [0.55934367, 0.46279101, 0.44880208, 1.        , 0.36588011,\n",
       "        0.49250584, 0.55922199, 0.48450068, 0.39109367, 0.09203781],\n",
       "       [0.5480876 , 0.59162586, 0.62986847, 0.36588011, 1.        ,\n",
       "        0.60955277, 0.53682121, 0.48266997, 0.4054681 , 0.05912017],\n",
       "       [0.61345432, 0.69162914, 0.6764548 , 0.49250584, 0.60955277,\n",
       "        1.        , 0.60010326, 0.54848904, 0.48150364, 0.13150585],\n",
       "       [0.65443752, 0.69906846, 0.63009485, 0.55922199, 0.53682121,\n",
       "        0.60010326, 1.        , 0.7086261 , 0.52208267, 0.0970571 ],\n",
       "       [0.64198519, 0.61477863, 0.55986681, 0.48450068, 0.48266997,\n",
       "        0.54848904, 0.7086261 , 1.        , 0.51335932, 0.17385641],\n",
       "       [0.64278306, 0.56881728, 0.50112936, 0.39109367, 0.4054681 ,\n",
       "        0.48150364, 0.52208267, 0.51335932, 1.        , 0.22865644],\n",
       "       [0.20997612, 0.10738494, 0.1148427 , 0.09203781, 0.05912017,\n",
       "        0.13150585, 0.0970571 , 0.17385641, 0.22865644, 1.        ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrmat = np.corrcoef(discrete_returns, rowvar=False)\n",
    "corrmat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us use this example to showcase some more advanced array selection. The correlation matrix is symmetric and we are interested in the values below or above the diagonal only, respectively. For instance, we may want to calculate how high the average pairwise correlation is. To get the indices of the lower triangular matrix, we can use the ```tril_indices``` method as shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices of lower triangular entries:\n",
      "\n",
      "(array([1, 2, 2, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 7,\n",
      "       7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "       9]), array([0, 0, 1, 0, 1, 2, 0, 1, 2, 3, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 5, 0,\n",
      "       1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 7, 0, 1, 2, 3, 4, 5, 6, 7,\n",
      "       8]))\n",
      "\n",
      "Corresponding correlations:\n",
      "\n",
      "[0.73390036 0.69870792 0.72875756 0.55934367 0.46279101 0.44880208\n",
      " 0.5480876  0.59162586 0.62986847 0.36588011 0.61345432 0.69162914\n",
      " 0.6764548  0.49250584 0.60955277 0.65443752 0.69906846 0.63009485\n",
      " 0.55922199 0.53682121 0.60010326 0.64198519 0.61477863 0.55986681\n",
      " 0.48450068 0.48266997 0.54848904 0.7086261  0.64278306 0.56881728\n",
      " 0.50112936 0.39109367 0.4054681  0.48150364 0.52208267 0.51335932\n",
      " 0.20997612 0.10738494 0.1148427  0.09203781 0.05912017 0.13150585\n",
      " 0.0970571  0.17385641 0.22865644]\n",
      "\n",
      "Average correlation: 0.4847\n"
     ]
    }
   ],
   "source": [
    "print(\"Indices of lower triangular entries:\\n\")\n",
    "print(np.tril_indices(corrmat.shape[0], k = -1))\n",
    "print(\"\")\n",
    "print(\"Corresponding correlations:\\n\")\n",
    "print(corrmat[np.tril_indices(corrmat.shape[0], k = -1)])\n",
    "print(\"\")\n",
    "print(f\"Average correlation: {np.mean(corrmat[np.tril_indices(corrmat.shape[0], k = -1)]):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Determine the average discrete return every day. This means we want to calculate the average of discrete returns per day (per row) which can be done by setting  ```axis = 1``` when using the *mean* method. Note that this corresponds to what is called a naive portfolio which means we split all our money equally among the companies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The discrete portfolio returns on the first five days: [-0.00724728 -0.03177535  0.00064376 -0.01111619  0.00053785]\n"
     ]
    }
   ],
   "source": [
    "discrete_returns_naive = discrete_returns.mean(axis = 1)\n",
    "print(f\"The discrete portfolio returns on the first five days: {discrete_returns_naive[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Transform to log-returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The log portfolio returns on the first five days: [-0.00727367 -0.03229115  0.00064355 -0.01117844  0.0005377 ]\n"
     ]
    }
   ],
   "source": [
    "log_returns_naive = np.log(1 + discrete_returns_naive)\n",
    "print(f\"The log portfolio returns on the first five days: {log_returns_naive[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Calculate the value development of one monetary unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(1.3259655751986106)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 * np.exp(log_returns_naive.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About 32\\% increase in value over the past two years. This sounds very promising. But note, by selecting today's largest company, we selected companies which must have been successful in the recent past. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear algebra with numpy\n",
    "\n",
    "Certain vector and matrix attributes are important in the field of finance and economics. Examples are:\n",
    "\n",
    "* linear independence of vectors\n",
    "* similarity of vectors\n",
    "* matrix inversion\n",
    "* matrix decomposition\n",
    "\n",
    "These examples are from the mathematical field of linear algebra. Numpy includes many useful linear algebra algorithms in the corresponding module. Let us use the examples from above to familiarize ourselves with this module.\n",
    "\n",
    "### Matrix multiplication\n",
    "\n",
    "When performing matrix multiplication in NumPy, there are several syntactic options available. While they often produce the same result for 2D arrays, there are important differences in behavior, readability, and support for higher-dimensional arrays. This section outlines the **similarities and differences** between the three primary methods:\n",
    "\n",
    "- `np.matmul(A, B)`\n",
    "- `A @ B`\n",
    "- `A.dot(B)`\n",
    "\n",
    "#### Similarities\n",
    "\n",
    "All three approaches perform **matrix multiplication**, not element-wise multiplication, when used with **2D arrays**.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.random.rand(4, 3)\n",
    "\n",
    "# All compute the same matrix product XᵀX\n",
    "version_1 = np.matmul(X.T, X)\n",
    "version_2 = X.T @ X\n",
    "version_3 = X.T.dot(X)\n",
    "\n",
    "# returns only True if all results are the same\n",
    "print((version_1 == version_2).all() and (version_2 == version_3).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these returns a `(3, 3)` matrix resulting from the product of `X.T` (shape `(3, 4)`) and `X` (shape `(4, 3)`).\n",
    "\n",
    "#### Differences\n",
    "\n",
    "| Feature                         | `np.matmul(A, B)`         | `A @ B`                       | `A.dot(B)`                    |\n",
    "|----------------------------------|----------------------------|-------------------------------|-------------------------------|\n",
    "| Syntax Type                     | Function                   | Operator                      | Method                        |\n",
    "| Supported array types           | 1D, 2D, ≥3D                | 1D, 2D, ≥3D                   | 1D, 2D only                   |\n",
    "| Supports batch multiplication   | ✅ Yes                     | ✅ Yes                         | ❌ No                         |\n",
    "| Readability (for 2D)            | Medium                     | ✅ Best                        | Acceptable                   |\n",
    "| Recommended for                 | High-dimensional matmul    | Clean matrix math (2D)        | Legacy, dot product use       |\n",
    "\n",
    "#### Special Considerations\n",
    "\n",
    "When working with 1D arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(11)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([1, 2])\n",
    "b = np.array([3, 4])\n",
    "\n",
    "a @ b        # 11: dot product\n",
    "a.dot(b)     # 11: dot product\n",
    "np.matmul(a, b)  # 11: dot product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "All methods behave the same for 1D vectors, returning a scalar (the inner product).\n",
    "\n",
    "For higher dimensions (Batch Matrix Multiplication) `np.matmul` and `@` support batch matrix multiplication for arrays with more than 2 dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the first matrix operation is: (10, 3, 2)\n",
      "The shape of the second matrix operation is: (10, 3, 2)\n",
      "The shape of the third matrix operation is: (10, 3, 10, 2)\n"
     ]
    }
   ],
   "source": [
    "A = np.random.rand(10, 3, 4)  # 10 matrices of shape (3, 4)\n",
    "B = np.random.rand(10, 4, 2)\n",
    "\n",
    "version_1 = np.matmul(A, B)  # (10, 3, 2)\n",
    "version_2 = A @ B                  # also works\n",
    "version_3 = A.dot(B)               # ❌ Error: not supported\n",
    "\n",
    "print(f\"The shape of the first matrix operation is: {version_1.shape}\")\n",
    "print(f\"The shape of the second matrix operation is: {version_2.shape}\")\n",
    "print(f\"The shape of the third matrix operation is: {version_3.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best Practice\n",
    "\n",
    "- Use `@` for **clean, readable matrix math** in 2D (e.g., linear regression, PCA).\n",
    "- Use `np.matmul()` when working with **3D+ arrays** (e.g., batch operations).\n",
    "- Avoid using `.dot()` for general matrix math — it does not support batching and can be misleading with 1D arrays.\n",
    "\n",
    "While `@`, `np.matmul`, and `.dot()` often produce the same results for 2D arrays, they differ in syntax, flexibility, and suitability for higher-dimensional computations. For modern, readable, and consistent matrix math, prefer the `@` operator or `np.matmul()` depending on dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear independence of vectors\n",
    "Vectors $\\boldsymbol{a_1}, \\boldsymbol{a_2}, ..., \\boldsymbol{a_k} \\in \\mathbb{R}^m$ are independent if the linear equation system:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "    a_{11} & \\cdots & a_{1n}\\\\\n",
    "    a_{21} & \\cdots & a_{2n}\\\\\n",
    "    \\vdots &        & \\vdots\\\\\n",
    "    a_{m1} & \\cdots & a_{mn}\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "    x_1 \\\\\n",
    "    x_2 \\\\\n",
    "    \\vdots \\\\\n",
    "    x_n \\\\\n",
    "\\end{pmatrix} = \n",
    "\\begin{pmatrix}\n",
    "    0 \\\\\n",
    "    0 \\\\\n",
    "    \\vdots \\\\\n",
    "    0 \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "only has a unique solution. If it exists it must be the so called trivial solution $\\boldsymbol{x} = \\boldsymbol{0}$. We can try the ```numpy.linalg.solve``` method and see if the solution exists and if it is the null vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An example matrix A: \n",
      "[[-1  1  1]\n",
      " [ 1 -3 -2]\n",
      " [ 5  1  4]]\n",
      "\n",
      "The solution for Ax=0: \n",
      "[ 0. -0.  0.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[-1, 1, 1], [1, -3, -2], [5, 1, 4]])\n",
    "print(f\"An example matrix A: \\n{A}\\n\")\n",
    "print(f\"The solution for Ax=0: \\n{np.linalg.solve(A, np.zeros(A.shape[0]))}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equivalently, we can calculate the determinant of A. If its value is different from zero, we know that the linear equation system has a unique solution. If it would be equal to zero, there may be multiple solutions or none."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The determinant of A: 12.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"The determinant of A: {np.linalg.det(A)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity of vectors\n",
    "\n",
    "Different ways exist to quantify how similar vectors are. Popular examples are euclidean distance and cosine similarity. Euclidean distance $d(\\boldsymbol{a}, \\boldsymbol{b})$ between two vectors $\\boldsymbol{a}, \\boldsymbol{b} \\in \\mathbb{R}^n$ is calculated as: \n",
    "\n",
    "$$d(\\boldsymbol{a}, \\boldsymbol{b}) = \\Vert \\boldsymbol{a} - \\boldsymbol{b} \\Vert = \t\\sqrt{\\sum_{i = 1}^{n} (a_i - b_i)^2}$$\n",
    "\n",
    "Cosine similarity measures similarity by the angle between two vectors and is derived by:\n",
    "\n",
    "$$\\cos \\theta = \\frac{\\boldsymbol{a}^T \\boldsymbol{b} }{ \\Vert \\boldsymbol{a} \\Vert \\Vert \\boldsymbol{b} \\Vert}$$\n",
    "\n",
    "The lower the distance or cosine similarity, respectively, the more similar the vectors are. Note that \n",
    "\n",
    "$$\\Vert\\boldsymbol{a}\\Vert=\\sqrt{\\boldsymbol{a^T\\ a}}=\\sqrt{\\sum\\limits_{i=1}^n a_i^2}$$\n",
    "\n",
    "is the l2-norm of vector $\\boldsymbol{a}$ and is calculated as the square root of the dot product of a vector with its transposed. Imagine, you collect financial indicators from companies and each vectors represents one company. With euclidean distance or cosine similarity you are able to analyze how similar companies are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example vector a: [2 1]\n",
      "Example vector b: [1 2]\n",
      "Euclidean distance between a and b: 1.4142\n",
      "Cosine similarity between a and b: 0.8000\n"
     ]
    }
   ],
   "source": [
    "a = np.array([2, 1])\n",
    "b = np.array([1, 2])\n",
    "\n",
    "print(f\"Example vector a: {a}\")\n",
    "print(f\"Example vector b: {b}\")\n",
    "print(f\"Euclidean distance between a and b: {np.linalg.norm(a - b):.4f}\")\n",
    "print(f\"Cosine similarity between a and b: {a.dot(b) / (np.linalg.norm(a) * np.linalg.norm(b)):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix inversion\n",
    "\n",
    "For a square matrix (number of rows and columns are equal), the inverse of a matrix $\\boldsymbol{A}^{-1}$ is defined as $\\boldsymbol{A} \\boldsymbol{A}^{-1} = \\boldsymbol{A}^{-1} \\boldsymbol{A} = \\boldsymbol{I}$. $\\boldsymbol{I}$ is called *identity matrix* and is a square matrix $\\boldsymbol{I} \\in \\mathbb{R}^{n \\times n}$ whose elements are all equal to zero except those on its diagonal which are all equal to one. However, not for every matrix an inverse exists, but if it exists it is unique; a matrix with an inverse is called *regular*, while it is called *singular* if no inverse exists. If a matrix is invertible can be examined by its determinant (must be different from zero) or if it has full rank which means the rank equals its rows or columns, respectively. The rank of a matrix is the number of independent rows or columns, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An example matrix A: \n",
      "[[-1  1  1]\n",
      " [ 1 -3 -2]\n",
      " [ 5  1  4]]\n",
      "\n",
      "Determinant of A: 12.0\n",
      "\n",
      "Inverse of A: \n",
      "[[-0.83 -0.25  0.08]\n",
      " [-1.17 -0.75 -0.08]\n",
      " [ 1.33  0.5   0.17]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[-1, 1, 1], [1, -3, -2], [5, 1, 4]])\n",
    "print(f\"An example matrix A: \\n{A}\\n\")\n",
    "print(f\"Determinant of A: {np.linalg.det(A)}\\n\")\n",
    "print(f\"Inverse of A: \\n{np.round(np.linalg.inv(A), 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix decomposition\n",
    "\n",
    "Sometimes decomposition of matrices can be useful for certain applications. We will take a look at the *eigendecomposition* as an example. The eigen decomposition of a square matrix $\\boldsymbol{A} \\in \\mathbb{R}^{n \\times n}$ depends on *eigenvalues* and *eigenvectors* of it. If a number $\\lambda \\in \\mathbb{R}$ exists such that:\n",
    "\n",
    "$$ \\boldsymbol{A} \\boldsymbol{x} = \\lambda \\boldsymbol{x} $$\n",
    "\n",
    "then we call $\\lambda$ eigenvalue and $\\boldsymbol{x} \\in \\mathbb{R}^n$ the eigenvector. As $\\boldsymbol{A} \\boldsymbol{0} = \\lambda \\boldsymbol{0}$ is always true, we are only interested in eigenvectors $\\boldsymbol{x} \\neq \\boldsymbol{0}$.\n",
    "\n",
    "We can rewrite the equation above to: \n",
    "\n",
    "$$ \\left( \\boldsymbol{A} - \\lambda \\boldsymbol{I}_n \\right) \\boldsymbol{x} = \\boldsymbol{0}$$\n",
    "\n",
    "One solution is always $\\boldsymbol{x} = \\boldsymbol{0}$ and as we are only interested in solutions different from this one, we are searching for values of $\\lambda$ such that multiple solutions exist. For this to be true $\\text{det}\\left( \\boldsymbol{A} - \\lambda \\boldsymbol{I}_n \\right) = 0$ must hold, because if the determinant is not equal to zero only a unique solution exists which is $\\boldsymbol{x} = \\boldsymbol{0}$. Deriving the determinant in dependence of $\\lambda$ leads to a $n$-th degree polynomial, the *characteristic polynomial*, whose roots are eigenvalues. Once we know those eigenvalues, we can derive corresponding eigenvectors with solving techniques of linear equation systems.\n",
    "\n",
    "Given a $n \\times n$ matrix has $n$ linearly independent eigenvectors with eigenvalues $\\boldsymbol{\\lambda} = (\\lambda_1, \\lambda_2, ..., \\lambda_n)$. The eigenvectors are concatenated column wise in the matrix:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{X} =  \n",
    "\\begin{pmatrix}\n",
    "   \\boldsymbol{x}^{(1)} & \\boldsymbol{x}^{(2)} & ... & \\boldsymbol{x}^{(n)} \\\\\n",
    "\\end{pmatrix} = \n",
    "\\begin{pmatrix}\n",
    "    x_{11} & \\cdots & a_{1n}\\\\\n",
    "    x_{21} & \\cdots & a_{2n}\\\\\n",
    "    \\vdots &        & \\vdots\\\\\n",
    "    x_{n1} & \\cdots & x_{nn}\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "then, the eigendecomposition of $\\boldsymbol{A}$ is given by:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{A} = \\boldsymbol{X}\\text{diag}\\lbrace \\lambda_i \\rbrace \\boldsymbol{X}^{-1}\n",
    "$$\n",
    "\n",
    "Eigenvectors can be used for many things and are needed for principal component analysis which is a tool for dimensionality reduction in data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example matrix A: \n",
      "[[1.   0.5  0.25]\n",
      " [0.5  1.   0.1 ]\n",
      " [0.25 0.1  1.  ]]\n",
      "\n",
      "Its eigenvalues: [1.6032748  0.47577635 0.92094885]\n",
      "\n",
      "Its eigenvectors (columnwise): \n",
      "[[ 0.67828342  0.72634911 -0.11112416]\n",
      " [ 0.62596041 -0.65037454 -0.43033304]\n",
      " [ 0.38484434 -0.22232844  0.89580405]]\n",
      "\n",
      "Calculation of its eigenvalue decomposition: \n",
      "[[1.   0.5  0.25]\n",
      " [0.5  1.   0.1 ]\n",
      " [0.25 0.1  1.  ]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[1, 0.5, 0.25], [0.5, 1, 0.1], [0.25, 0.1, 1]])\n",
    "\n",
    "print(f\"Example matrix A: \\n{A}\\n\")\n",
    "\n",
    "l, X = np.linalg.eig(A)\n",
    "print(f\"Its eigenvalues: {l}\\n\")\n",
    "print(f\"Its eigenvectors (columnwise): \\n{X}\\n\")\n",
    "print(f\"Calculation of its eigenvalue decomposition: \\n{X.dot(np.diag(l)).dot(np.linalg.inv(X))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression with numpy\n",
    "\n",
    "One of the most popular methods in the financial domain is the linear regression model. To estimate its parameters, we only need to be able to conduct basic matrix operations as we learned them in the previous subsection.\n",
    "\n",
    "The linear regression model is defined by:\n",
    "\n",
    "$$\n",
    "y = x_1 \\beta_1 + ... + x_p \\beta_p + \\epsilon = \\boldsymbol{x}^T \\boldsymbol{\\beta} + \\epsilon\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "* $y$ is the dependent variable\n",
    "* $\\boldsymbol{x}$ are independent variables\n",
    "* $\\boldsymbol{\\beta}$ are parameters of the regression line\n",
    "* $\\epsilon$ is the residual term which is assumed to be a random variable with variance $\\sigma^2$\n",
    "\n",
    "In order to estimated the model parameters, we can use ordinary least square estimation which minimizes the score function:\n",
    "\n",
    "$$\n",
    "S(\\boldsymbol{\\beta}) = \\sum_{i=1}^n \\left( y_i - \\boldsymbol{x}_i^T \\boldsymbol{\\beta} \\right)^2 = || \\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta}||^2 \n",
    "$$\n",
    "\n",
    "The solution to this minimization problem is given by:\n",
    "\n",
    "$$\n",
    "\\hat{\\boldsymbol{\\beta}} = \\left( \\boldsymbol{X}^T \\boldsymbol{X} \\right)^{-1} \\boldsymbol{X}^T \\boldsymbol{y}\n",
    "$$\n",
    "\n",
    "Given these estimates, we can calculate:\n",
    "\n",
    "$$\n",
    "\\hat{\\boldsymbol{\\epsilon}} = \\boldsymbol{y} - \\boldsymbol{x}_i^T \\hat{\\boldsymbol{\\beta}}\n",
    "$$\n",
    "\n",
    "And estimate the last parameter of the model with:\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}^2 = \\frac{\\hat{\\boldsymbol{\\epsilon}}^T\\hat{\\boldsymbol{\\epsilon}}}{n - p}\n",
    "$$\n",
    "\n",
    "Note that $n$ represents the number of observations and $p$ the number of independent variables, $\\left(n - p\\right)$ in the denominator is a bias correction. The linear regression model is well implemented in libraries such as scikit-learn or statsmodels. However, for didactic reasons, we are going to implement our own small regression class which can be done with the knowledge you gained so far. Take a look at the class function below which you are going to learn in detail in the tutorial of this course. Examples for linear regression analysis on financial markets are cross-sectional regressions of asset price returns as well as factor regressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinearRegression:\n",
    "\n",
    "    # add_constant clarifies if we want to estimate the regression line with our without constant \\beta_0\n",
    "    def __init__(self, add_constant = False):\n",
    "        self.add_constant = add_constant\n",
    "\n",
    "    # we try to mimic the convention of the scikit-learn library which implements a fit method for the estimation of model parameters\n",
    "    def fit(self, X, y):\n",
    "        if len(X.shape) == 1:\n",
    "            X = X.reshape(-1, 1)\n",
    "        if self.add_constant:\n",
    "            self.X = np.concatenate((np.ones((X.shape[0], 1)), X), axis = 1)\n",
    "        else:\n",
    "            self.X = X\n",
    "        self.y = y\n",
    "\n",
    "        # formula for the OLS method\n",
    "        self.beta_coef = np.matmul(np.linalg.inv(np.matmul(self.X.transpose(), self.X)), self.X.transpose()).dot(self.y)\n",
    "        # determine the predictions for given data\n",
    "        self.y_hat = self.X.dot(self.beta_coef)\n",
    "        # determine epsilon_hat for every observation\n",
    "        self.residuals = self.y - self.y_hat\n",
    "        # estimate the variance\n",
    "        self.var_coef = self.residuals.transpose().dot(self.residuals) / (self.X.shape[0] - self.X.shape[1])\n",
    "        # accordingly, the estimated standard deviation\n",
    "        self.sd_coef = np.sqrt(self.var_coef)\n",
    "\n",
    "    # in the tutorial we may want to add another method which predicts values for new and unseen data\n",
    "    def predict(self, X):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One very important question for financial markets theory, i.e., for asset pricing, is the link of a company's return to systematic risk factors because it tells us how large expected returns should be, given investors bear the related risk. This goes back to the famous capital asset pricing model which assumes that returns (more concrete excess returns over the risk free rate) can be explained by their linkage to the market portfolio. The according regression model regresses the excess return of a security upon the excess return of a market portfolio:\n",
    "\n",
    "$$r_{it} - r_{ft} = \\beta_0 + \\beta_1 r_{mt} - r_{ft} + \\epsilon_{it}$$\n",
    "\n",
    "$r_{it}, r_{mt}, r_{ft}$ represent the security, market and risk free return at time $t$. The higher $\\beta_1$ the more sensitive reacts the company towards market movements, the more it correlates which are companies that are also sensitive to the market. While this may lead to higher upswings of the market value in good economic times, it also leads to larger drops in times of financial turmoil. This behavior increases the variation in market value and, thus, the volatility of the return. As volatility is considered to be a good approximation of risk on financial markets (due to risk aversion of rational investors), investors should be compensated with higher profits. To estimate the $\\beta_1$ for each company, we can use our own class and some example data. \n",
    "\n",
    "See the cell below for a short demonstration which uses data from the past two years, omits the risk free rate and assumes the Russell 3000 index to be a good approximation of the market portfolio. The results show us that these companies mostly exhibit estimates $\\hat{\\beta}_1 > 1$ which speaks for a sensitive behavior towards the overall market. Note that if the exposure to the market is enough to price the asset appropriately, $\\beta_0$ should be equal to zero. Empirically, it has been shown that further risk factors should be taken into account. We come to this at a later point in time in the course. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First return observations in the data set:\n",
      "\n",
      "                 RUA      AAPL      MSFT     GOOGL      TSLA      META  \\\n",
      "2022-01-04 -0.001611 -0.012692 -0.017147 -0.004083 -0.041833 -0.005937   \n",
      "2022-01-05 -0.021807 -0.026600 -0.038388 -0.045876 -0.053471 -0.036728   \n",
      "2022-01-06 -0.000051 -0.016693 -0.007902 -0.000200 -0.021523  0.025573   \n",
      "2022-01-07 -0.004965  0.000988  0.000510 -0.005303 -0.035447 -0.002015   \n",
      "2022-01-10 -0.001460  0.000116  0.000732  0.012061  0.030342 -0.011212   \n",
      "\n",
      "                AMZN      NVDA      AVGO         V       XOM  \n",
      "2022-01-04 -0.016916 -0.027589  0.011457  0.004652  0.037614  \n",
      "2022-01-05 -0.018893 -0.057562 -0.041614 -0.011058  0.012437  \n",
      "2022-01-06 -0.006711  0.020794 -0.009285 -0.001136  0.023521  \n",
      "2022-01-07 -0.004288 -0.033040 -0.028068 -0.012696  0.008197  \n",
      "2022-01-10 -0.006570  0.005615  0.003246 -0.023000 -0.005952  \n",
      "\n",
      "Estimated market betas: \n",
      "[1.2  1.2  1.3  1.83 1.73 1.58 2.11 1.3  0.88 0.51]\n"
     ]
    }
   ],
   "source": [
    "# import adjusted close prices\n",
    "close_prices = pd.read_csv(\"../data/close_prices_small.csv\", index_col = \"date\")\n",
    "close_prices_rua = pd.read_csv(\"../data/close_prices_rua.csv\", index_col=\"Date\")\n",
    "close_prices_all = close_prices_rua[\"Adj Close\"].to_frame(\"RUA\").merge(close_prices, left_index = True, right_index = True)\n",
    "# calculate returns \n",
    "discrete_returns = close_prices_all.pct_change().dropna()\n",
    "print(\"First return observations in the data set:\\n\")\n",
    "print(discrete_returns.head())\n",
    "\n",
    "# define independent and dependent variables\n",
    "X = discrete_returns.RUA.values.reshape(-1, 1)\n",
    "Y = discrete_returns.drop([\"RUA\"], axis = 1).values\n",
    "\n",
    "# in a loop estimate betas with our own class\n",
    "betas = []\n",
    "for i in range(Y.shape[1]):\n",
    "    lr = MyLinearRegression(add_constant=True)\n",
    "    lr.fit(X, Y[:,i])\n",
    "    betas.append(lr.beta_coef[1])\n",
    "\n",
    "print(f\"\\nEstimated market betas: \\n{np.round(betas, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA with numpy\n",
    "\n",
    "Let us take a look into another interesting method which can be used for financial analysis and is linked to the field of linear algebra - *principal component analysis* (PCA). Generally, PCA receives mean centered data $\\boldsymbol{X} \\in \\mathbb{R}^{n \\times p}$ where $n$ is the number of observations and $p$ the number of variables which are often also called features. PCA creates up to $p$ new variables out of the data by creating linear combinations of the form:\n",
    "\n",
    "$$ z_{ij} = \\boldsymbol{x}_i^T \\boldsymbol{\\phi}_j = x_{i1} \\phi_{j1} + ... + x_{ip} \\phi_{jp} $$\n",
    "\n",
    "$z_{ij}$ is the principal component score for observation $i$ and pricipal component $j$. $\\boldsymbol{\\phi}_j$ is represents principal component $j$ where its entries are often called loadings. You may realize that each principal component creates a weighted average of all observable realizations for each observation. Each principal component is normalized, thus, $||\\boldsymbol{\\phi}|| = 1$ and independent to the other principal components. Its most common application is in the field of dimensionality reduction. This means the aim is to create $m < p$ variables out of the original data which keep the most important information. Two equivalent optimization problems lead to the best values of $ \\boldsymbol{\\Phi} =  \\begin{pmatrix} \\boldsymbol{\\phi}_1 &  ... & \\boldsymbol{\\phi}_m \\\\ \\end{pmatrix}$ for given data. One can either minimize the difference between actual observations and their projections:\n",
    "\n",
    "$$ \n",
    "||\\boldsymbol{X} - \\boldsymbol{X} \\boldsymbol{\\Phi} \\boldsymbol{\\Phi}^T ||^2\n",
    "$$\n",
    "\n",
    "or maximize the variance of all pca score vectors $\\boldsymbol{z}_j$ always subject to $\\boldsymbol{\\Phi}^T \\boldsymbol{\\Phi} = \\boldsymbol{I}^{m \\times m}$ (orthonormality). The solutions to these optimization problems can be found by calculating the eigenvectors of the covariance maxtrix from $\\boldsymbol{X}$. As it is a positive definit matrix, its eigenvalues are all positive. Each eigenvalue corresponds to a principal component and the eigenvalue tells us how much of the variation is captured by the corresponding principal component. Instead of the covariance matrix, we can also use the correlation matrix. This leads to different results, but, shifts the focus towards pairwise correlations instead of univariate variation. In a financial context, this is often more important. Let us take a look in the cell below which determines the eigenvectors and eigenvalues for the correlation matrix or four companies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First five observations of discrete returns for Apple, Microsoft, Google and Tesla:\n",
      "\n",
      "                AAPL      MSFT     GOOGL      TSLA\n",
      "date                                              \n",
      "2022-01-04 -0.012692 -0.017147 -0.004083 -0.041833\n",
      "2022-01-05 -0.026600 -0.038388 -0.045876 -0.053471\n",
      "2022-01-06 -0.016693 -0.007902 -0.000200 -0.021523\n",
      "2022-01-07  0.000988  0.000510 -0.005303 -0.035447\n",
      "2022-01-10  0.000116  0.000732  0.012061  0.030342\n",
      "\n",
      "correlation matrix: \n",
      "[[1.   0.73 0.7  0.56]\n",
      " [0.73 1.   0.73 0.46]\n",
      " [0.7  0.73 1.   0.45]\n",
      " [0.56 0.46 0.45 1.  ]]\n",
      "\n",
      "Eigenvalues: [2.83 0.25 0.29 0.63]\n",
      "\n",
      "Eigenvectors: \n",
      "[[ 0.53  0.61  0.58 -0.05]\n",
      " [ 0.52 -0.74  0.28 -0.32]\n",
      " [ 0.51  0.23 -0.75 -0.35]\n",
      " [ 0.42 -0.14 -0.16  0.88]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "close_prices = pd.read_csv(\"../data/close_prices_small.csv\", index_col = \"date\")\n",
    "close_prices = close_prices.loc[:, [\"AAPL\", \"MSFT\", \"GOOGL\", \"TSLA\"]]\n",
    "discrete_returns = close_prices.pct_change().dropna()\n",
    "print(\"First five observations of discrete returns for Apple, Microsoft, Google and Tesla:\\n\")\n",
    "print(discrete_returns.head())\n",
    "X = discrete_returns.values\n",
    "corrmat = np.corrcoef(X, rowvar = False)\n",
    "print(f\"\\ncorrelation matrix: \\n{np.round(corrmat, 2)}\\n\")\n",
    "v, L = np.linalg.eig(corrmat)\n",
    "print(f\"Eigenvalues: {np.round(v, 2)}\\n\")\n",
    "print(f\"Eigenvectors: \\n{np.round(L, 2)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first principal component has the largest eigenvalue ($2.83$) and, thus, captures the largest amount of variation in the original data. For this vector, the loadings of Apple and Microsoft are the largest which underlines their importance for co-movements of the companies' returns. At the same time Tesla has the smallest loading which makes sense as it exhibits the lowest correlation on average to the remaining companies. All companies share the same (positive) sign w.r.t. to the loading of the first principal component. This means if the value of $z_{i1}$ is positive, we simply know by this value that on average this must have been a day with positive market value developments. The second largest eigenvalue can be observed for the fourth eigenvector. Looking at the signs of the loadings we observe that the second principle component splits Tesla apart from the other companies. This seems to make sense, given the different business in comparison to the remaining companies. How many principal components we want to use, depends on a trade-off between dimensionality and variation explained by the lower dimensional representation. Determining the proportion of each eigenvalue and the sum of all eigenvalues we can extract the variation explained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variation explained by each principal component in decreasing order:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.70865207, 0.15637343, 0.07222124, 0.06275327])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Variation explained by each principal component in decreasing order:\")\n",
    "np.flip(np.sort(v)) / v.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, a little more than 85\\% of variation is explained by the first two principal components. This means by just two variables, we contain a large amount of variation in the original data. However, with only two dimensions, we, e.g., would be able to visualize the data in a two dimensional plot. Furthermore, another important application would be to use the first principal components and their scores as systematic risk factors for a regression type analysis as for the regression example above. The estimated betas can be used to examine which companies systematically behave similar and different to others, respectively. Such an information can be used, e.g., for portfolio selection purposes and to identify systematically vulnerable companies. Let us end this subsection with an example class which would implement PCA by using numpy only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyPCA:\n",
    "    def __init__(self, corrmat = True):\n",
    "        self.corrmat = corrmat\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.X = X\n",
    "        if self.corrmat:\n",
    "            self.X_ = None\n",
    "            self.mat = np.corrcoef(X, rowvar = False)\n",
    "        else:\n",
    "            self.X_ = self.X - self.X.mean(axis = 0)\n",
    "            self.mat = np.cov(self.X_, rowvar = False)\n",
    "        v, L = np.linalg.eig(self.mat)\n",
    "\n",
    "        self.eigenvalues = v[np.flip(np.argsort(v))]\n",
    "        self.loadings = L[:, np.flip(np.argsort(v))]\n",
    "\n",
    "        self.variation_explained = self.eigenvalues / self.eigenvalues.sum()\n",
    "        self.variation_explained_cumsum = np.cumsum(self.variation_explained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chapter is meant to introduce and highlight some functionalities of the numpy package. On purpose, this chapter has a rather raw looking output. The next chapters will introduce packages for data manipulation and visualization to increase accessability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
